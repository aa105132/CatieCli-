"""
OpenAI Codex API å®¢æˆ·ç«¯

å®ç° Codex API è°ƒç”¨ï¼Œå°†è¯·æ±‚å‘é€åˆ° chatgpt.com/backend-api/codex/responses
å¹¶è½¬æ¢å“åº”æ ¼å¼ä¸º OpenAI Chat Completions æ ¼å¼ã€‚
"""

import json
import time
import uuid
import httpx
from typing import AsyncGenerator, Dict, Any, List, Optional, Tuple
from app.services.codex_auth import get_codex_headers, CODEX_API_BASE


# æ¨¡å‹åç¼€é…ç½®
# -maxthinking: æœ€é«˜æ¨ç†å¼ºåº¦ (xhigh)
# -nothinking: æœ€ä½æ¨ç†å¼ºåº¦ (minimal)
# -low: ä½æ¨ç†å¼ºåº¦
MODEL_SUFFIXES = {
    "-maxthinking": "xhigh",
    "-nothinking": "minimal",
    "-low": "low",
}

# æ”¯æŒåç¼€çš„åŸºç¡€æ¨¡å‹åˆ—è¡¨ï¼ˆåŒ…æ‹¬æ‰€æœ‰ GPT-5.x ç³»åˆ—ï¼‰
MODELS_WITH_THINKING = [
    "gpt-5",
    "gpt-5.1",
    "gpt-5.2",
    "gpt-5-codex",
    "gpt-5.1-codex",
    "gpt-5.2-codex",
    "gpt-5-codex-mini",
    "gpt-5.1-codex-mini",
    "gpt-5.1-codex-max",
]


def parse_model_suffix(model: str) -> tuple:
    """
    è§£ææ¨¡å‹åç§°å’Œåç¼€
    
    Returns:
        (base_model, reasoning_effort)
        
    Example:
        "gpt-5.2-maxthinking" -> ("gpt-5.2", "high")
        "gpt-5.1-high" -> ("gpt-5.1", "high")
        "gpt-4.1-mini" -> ("gpt-4.1-mini", "medium")
    """
    model_lower = model.lower()
    
    for suffix, effort in MODEL_SUFFIXES.items():
        if model_lower.endswith(suffix):
            base_model = model[:-len(suffix)]
            # æ£€æŸ¥åŸºç¡€æ¨¡å‹æ˜¯å¦æ”¯æŒæ€ç»´é“¾åç¼€
            for supported in MODELS_WITH_THINKING:
                if base_model.lower().startswith(supported) or base_model.lower() == supported:
                    return base_model, effort
            # ä¸æ”¯æŒçš„æ¨¡å‹ï¼Œç›´æ¥è¿”å›åŸæ¨¡å‹å
            return base_model, effort
    
    return model, "medium"  # é»˜è®¤ medium


class CodexClient:
    """Codex API å®¢æˆ·ç«¯"""
    
    def __init__(self, access_token: str, account_id: str = ""):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯
        
        Args:
            access_token: OAuth access token
            account_id: ChatGPT account ID
        """
        self.access_token = access_token
        self.account_id = account_id
        self.api_base = CODEX_API_BASE
    
    def _get_headers(self) -> Dict[str, str]:
        """è·å–è¯·æ±‚å¤´"""
        return get_codex_headers(self.access_token, self.account_id)
    
    def _convert_messages_to_input(self, messages: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], str]:
        """
        å°† OpenAI æ¶ˆæ¯æ ¼å¼è½¬æ¢ä¸º Codex input æ ¼å¼
        
        OpenAI æ ¼å¼:
        [{"role": "user", "content": "Hello"}]
        
        Codex æ ¼å¼:
        [{"type": "message", "role": "user", "content": [{"type": "input_text", "text": "Hello"}]}]
        
        æ³¨æ„:
        1. system è§’è‰²è½¬æ¢ä¸º developer è§’è‰²
        2. user æ¶ˆæ¯å†…å®¹ä½¿ç”¨ input_text ç±»å‹
        3. assistant æ¶ˆæ¯å†…å®¹ä½¿ç”¨ output_text ç±»å‹
        4. æå– instructionsï¼ˆä»ç³»ç»Ÿæ¶ˆæ¯ï¼‰
        
        Returns:
            Tuple[input åˆ—è¡¨, instructions å­—ç¬¦ä¸²]
        """
        result = []
        instructions = ""
        
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            # æå– system æ¶ˆæ¯ä½œä¸º instructions
            if role == "system":
                if isinstance(content, str):
                    instructions = content
                elif isinstance(content, list):
                    for part in content:
                        if isinstance(part, str):
                            instructions += part
                        elif isinstance(part, dict) and part.get("type") == "text":
                            instructions += part.get("text", "")
                # system æ¶ˆæ¯è½¬æ¢ä¸º developer è§’è‰²
                role = "developer"
            
            # ç¡®å®š content ç±»å‹ï¼ˆåŸºäºè§’è‰²ï¼‰
            text_type = "output_text" if role == "assistant" else "input_text"
            
            # å¤„ç† content æ ¼å¼
            if isinstance(content, str):
                content_parts = [{"type": text_type, "text": content}]
            elif isinstance(content, list):
                # å·²ç»æ˜¯æ•°ç»„æ ¼å¼ï¼ˆå¯èƒ½åŒ…å«å›¾ç‰‡ç­‰ï¼‰
                content_parts = []
                for part in content:
                    if isinstance(part, str):
                        content_parts.append({"type": text_type, "text": part})
                    elif isinstance(part, dict):
                        if part.get("type") == "text":
                            content_parts.append({"type": text_type, "text": part.get("text", "")})
                        elif part.get("type") == "image_url":
                            # Codex æ”¯æŒå›¾ç‰‡è¾“å…¥
                            content_parts.append({
                                "type": "input_image",
                                "image_url": part.get("image_url", {}).get("url", "")
                            })
                        else:
                            content_parts.append(part)
            else:
                content_parts = [{"type": text_type, "text": str(content)}]
            
            # å¤„ç† tool_calls
            if role == "assistant" and "tool_calls" in msg:
                # å…ˆæ·»åŠ åŠ©æ‰‹æ¶ˆæ¯ï¼ˆå¦‚æœæœ‰å†…å®¹ï¼‰
                if content_parts and content_parts[0].get("text"):
                    result.append({
                        "type": "message",
                        "role": role,
                        "content": content_parts,
                    })
                # åŠ©æ‰‹æ¶ˆæ¯å¸¦å·¥å…·è°ƒç”¨ - ä½œä¸ºé¡¶å±‚å¯¹è±¡
                for tc in msg["tool_calls"]:
                    result.append({
                        "type": "function_call",
                        "call_id": tc.get("id", str(uuid.uuid4())),
                        "name": tc.get("function", {}).get("name", ""),
                        "arguments": tc.get("function", {}).get("arguments", ""),
                    })
            elif role == "tool":
                # å·¥å…·å“åº” - ä½œä¸ºé¡¶å±‚å¯¹è±¡
                tool_output = ""
                if isinstance(content, str):
                    tool_output = content
                elif content_parts and content_parts[0].get("text"):
                    tool_output = content_parts[0].get("text", "")
                    
                result.append({
                    "type": "function_call_output",
                    "call_id": msg.get("tool_call_id", ""),
                    "output": tool_output,
                })
            else:
                # æ™®é€šæ¶ˆæ¯
                result.append({
                    "type": "message",
                    "role": role,
                    "content": content_parts,
                })
        
        return result, instructions
    
    def _convert_tools_to_codex(self, tools: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å°† OpenAI tools æ ¼å¼è½¬æ¢ä¸º Codex æ ¼å¼"""
        result = []
        for tool in tools:
            if tool.get("type") == "function":
                func = tool.get("function", {})
                result.append({
                    "type": "function",
                    "name": func.get("name", ""),
                    "description": func.get("description", ""),
                    "parameters": func.get("parameters", {}),
                })
        return result
    
    def _build_request_body(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        tools: Optional[List[Dict[str, Any]]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        æ„å»º Codex API è¯·æ±‚ä½“
        
        åŸºäº CLIProxyAPI çš„å®ç°ï¼ŒCodex API æœ‰ç‰¹æ®Šçš„è¯·æ±‚æ ¼å¼ï¼š
        - instructions: ç³»ç»ŸæŒ‡ä»¤ï¼ˆä» system æ¶ˆæ¯æå–ï¼‰
        - input: æ¶ˆæ¯åˆ—è¡¨
        - reasoning: æ¨ç†é…ç½®
        - store: falseï¼ˆä¸å­˜å‚¨ï¼‰
        
        æ”¯æŒæ¨¡å‹åç¼€:
        - gpt-5.2-maxthinking â†’ reasoning.effort = "high"
        - gpt-5.1-high â†’ reasoning.effort = "high"
        """
        # è§£ææ¨¡å‹åç¼€
        base_model, parsed_effort = parse_model_suffix(model)
        
        # ä¼˜å…ˆä½¿ç”¨ kwargs ä¸­çš„ reasoning_effortï¼Œå¦åˆ™ä½¿ç”¨è§£æçš„åç¼€
        reasoning_effort = kwargs.get("reasoning_effort", parsed_effort)
        
        # è½¬æ¢æ¶ˆæ¯å¹¶æå– instructions
        input_list, instructions = self._convert_messages_to_input(messages)
        
        body = {
            "model": base_model,  # ä½¿ç”¨å»æ‰åç¼€çš„åŸºç¡€æ¨¡å‹å
            "input": input_list,
            "stream": True,  # Codex é»˜è®¤ä½¿ç”¨æµå¼
            "instructions": instructions,  # ä» system æ¶ˆæ¯æå–çš„æŒ‡ä»¤
            "store": False,  # ä¸å­˜å‚¨
            "parallel_tool_calls": True,
            "reasoning": {
                "effort": reasoning_effort,
                "summary": "auto",
            },
            "include": ["reasoning.encrypted_content"],
        }
        
        if tools:
            body["tools"] = self._convert_tools_to_codex(tools)
        
        # Codex ä¸æ”¯æŒ temperature, top_p, max_tokens ç­‰å‚æ•°ï¼ˆæ ¹æ® CLIProxyAPI å®ç°ï¼‰
        # è¿™äº›å‚æ•°è¢«æ³¨é‡Šæ‰äº†ï¼Œä¿æŒæ³¨é‡Šä»¥ä¾¿å°†æ¥å‚è€ƒ
        # if "temperature" in kwargs:
        #     body["temperature"] = kwargs["temperature"]
        # if "max_tokens" in kwargs:
        #     body["max_output_tokens"] = kwargs["max_tokens"]
        # if "top_p" in kwargs:
        #     body["top_p"] = kwargs["top_p"]
        
        return body
    
    async def chat_completions(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        tools: Optional[List[Dict[str, Any]]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        éæµå¼ Chat Completions
        
        å†…éƒ¨ä½¿ç”¨æµå¼ APIï¼Œæ”¶é›†å®Œæ•´å“åº”åè¿”å›
        """
        full_content = ""
        reasoning_content = ""
        collected_tool_calls = {}
        usage = None
        
        async for chunk in self.chat_completions_stream(model, messages, tools, **kwargs):
            if chunk.startswith("data: "):
                data = chunk[6:]
                if data.strip() == "[DONE]":
                    continue
                try:
                    chunk_json = json.loads(data)
                    if "choices" in chunk_json and chunk_json["choices"]:
                        choice = chunk_json["choices"][0]
                        delta = choice.get("delta", {})
                        
                        if "content" in delta:
                            full_content += delta["content"]
                        if "reasoning_content" in delta:
                            reasoning_content += delta["reasoning_content"]
                        
                        # æ”¶é›†å·¥å…·è°ƒç”¨
                        if "tool_calls" in delta:
                            for tc in delta["tool_calls"]:
                                idx = tc.get("index", 0)
                                if idx not in collected_tool_calls:
                                    collected_tool_calls[idx] = {
                                        "id": tc.get("id", f"call_{idx}"),
                                        "type": "function",
                                        "function": {
                                            "name": tc.get("function", {}).get("name", ""),
                                            "arguments": tc.get("function", {}).get("arguments", "")
                                        }
                                    }
                                else:
                                    if "function" in tc:
                                        func = tc["function"]
                                        if "name" in func and func["name"]:
                                            collected_tool_calls[idx]["function"]["name"] = func["name"]
                                        if "arguments" in func:
                                            collected_tool_calls[idx]["function"]["arguments"] += func["arguments"]
                    
                    if "usage" in chunk_json:
                        usage = chunk_json["usage"]
                except json.JSONDecodeError:
                    pass
        
        # æ„å»ºå“åº”
        message = {"role": "assistant"}
        
        if collected_tool_calls:
            message["tool_calls"] = [collected_tool_calls[i] for i in sorted(collected_tool_calls.keys())]
            message["content"] = full_content if full_content else None
            finish_reason = "tool_calls"
        else:
            message["content"] = full_content
            finish_reason = "stop"
        
        if reasoning_content:
            message["reasoning_content"] = reasoning_content
        
        return {
            "id": f"chatcmpl-codex-{uuid.uuid4().hex[:8]}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": message,
                "finish_reason": finish_reason
            }],
            "usage": usage or {
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0
            }
        }
    
    async def chat_completions_stream(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        tools: Optional[List[Dict[str, Any]]] = None,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼ Chat Completions
        
        å°† Codex å“åº”è½¬æ¢ä¸º OpenAI SSE æ ¼å¼
        """
        url = f"{self.api_base}/responses"
        body = self._build_request_body(model, messages, tools, **kwargs)
        headers = self._get_headers()
        
        request_id = uuid.uuid4().hex[:8]
        print(f"[Codex Client] ğŸš€ è¯·æ±‚ {request_id}: model={model}, url={url}", flush=True)
        
        try:
            async with httpx.AsyncClient(timeout=300.0) as client:
                async with client.stream("POST", url, json=body, headers=headers) as response:
                    if response.status_code != 200:
                        error_body = await response.aread()
                        print(f"[Codex Client] âŒ è¯·æ±‚ {request_id} å¤±è´¥: {response.status_code} - {error_body[:500]}", flush=True)
                        raise Exception(f"Codex API Error {response.status_code}: {error_body.decode()[:500]}")
                    
                    buffer = ""
                    async for chunk in response.aiter_text():
                        buffer += chunk
                        
                        while "\n" in buffer:
                            line, buffer = buffer.split("\n", 1)
                            line = line.strip()
                            
                            if not line:
                                continue
                            
                            if line.startswith("data: "):
                                data = line[6:]
                                if data == "[DONE]":
                                    yield "data: [DONE]\n\n"
                                    return
                                
                                try:
                                    event = json.loads(data)
                                    translated = self._translate_event(event, model, request_id)
                                    if translated:
                                        yield f"data: {json.dumps(translated)}\n\n"
                                except json.JSONDecodeError:
                                    pass
                    
                    # å¤„ç†ç¼“å†²åŒºå‰©ä½™å†…å®¹
                    if buffer.strip():
                        if buffer.startswith("data: "):
                            data = buffer[6:].strip()
                            if data and data != "[DONE]":
                                try:
                                    event = json.loads(data)
                                    translated = self._translate_event(event, model, request_id)
                                    if translated:
                                        yield f"data: {json.dumps(translated)}\n\n"
                                except json.JSONDecodeError:
                                    pass
                    
                    yield "data: [DONE]\n\n"
        
        except httpx.TimeoutException:
            raise Exception("Codex API è¯·æ±‚è¶…æ—¶")
        except httpx.RequestError as e:
            raise Exception(f"Codex API ç½‘ç»œé”™è¯¯: {str(e)}")
    
    def _translate_event(self, event: Dict[str, Any], model: str, request_id: str) -> Optional[Dict[str, Any]]:
        """
        å°† Codex äº‹ä»¶è½¬æ¢ä¸º OpenAI Chat Completions æµå¼æ ¼å¼
        """
        event_type = event.get("type", "")
        
        # å¤„ç†ä¸åŒçš„äº‹ä»¶ç±»å‹
        if event_type == "response.output_item.added":
            # è¾“å‡ºé¡¹æ·»åŠ ï¼ˆå¦‚å¼€å§‹ç”Ÿæˆæ¶ˆæ¯ï¼‰
            return None
        
        elif event_type == "response.content_part.added":
            # å†…å®¹éƒ¨åˆ†æ·»åŠ 
            return None
        
        elif event_type == "response.output_text.delta":
            # æ–‡æœ¬å¢é‡
            delta_text = event.get("delta", "")
            if delta_text:
                return {
                    "id": f"chatcmpl-codex-{request_id}",
                    "object": "chat.completion.chunk",
                    "created": int(time.time()),
                    "model": model,
                    "choices": [{
                        "index": 0,
                        "delta": {"content": delta_text},
                        "finish_reason": None
                    }]
                }
        
        elif event_type == "response.reasoning_summary_text.delta":
            # æ€ç»´é“¾å¢é‡ï¼ˆä½¿ç”¨æ­£ç¡®çš„äº‹ä»¶ç±»å‹ï¼‰
            delta_text = event.get("delta", "")
            if delta_text:
                return {
                    "id": f"chatcmpl-codex-{request_id}",
                    "object": "chat.completion.chunk",
                    "created": int(time.time()),
                    "model": model,
                    "choices": [{
                        "index": 0,
                        "delta": {"reasoning_content": delta_text},
                        "finish_reason": None
                    }]
                }
        
        elif event_type == "response.reasoning_summary_text.done":
            # æ€ç»´é“¾å®Œæˆï¼Œæ·»åŠ æ¢è¡Œ
            return {
                "id": f"chatcmpl-codex-{request_id}",
                "object": "chat.completion.chunk",
                "created": int(time.time()),
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {"reasoning_content": "\n\n"},
                    "finish_reason": None
                }]
            }
        
        elif event_type == "response.function_call_arguments.delta":
            # å·¥å…·è°ƒç”¨å‚æ•°å¢é‡
            delta_args = event.get("delta", "")
            call_id = event.get("call_id", "")
            item_id = event.get("item_id", "")
            if delta_args:
                return {
                    "id": f"chatcmpl-codex-{request_id}",
                    "object": "chat.completion.chunk",
                    "created": int(time.time()),
                    "model": model,
                    "choices": [{
                        "index": 0,
                        "delta": {
                            "tool_calls": [{
                                "index": 0,
                                "id": call_id or item_id,
                                "type": "function",
                                "function": {"arguments": delta_args}
                            }]
                        },
                        "finish_reason": None
                    }]
                }
        
        elif event_type == "response.function_call_arguments.done":
            # å·¥å…·è°ƒç”¨å®Œæˆ
            name = event.get("name", "")
            call_id = event.get("call_id", "")
            if name:
                return {
                    "id": f"chatcmpl-codex-{request_id}",
                    "object": "chat.completion.chunk",
                    "created": int(time.time()),
                    "model": model,
                    "choices": [{
                        "index": 0,
                        "delta": {
                            "tool_calls": [{
                                "index": 0,
                                "id": call_id,
                                "type": "function",
                                "function": {"name": name}
                            }]
                        },
                        "finish_reason": None
                    }]
                }
        
        elif event_type == "response.completed":
            # å“åº”å®Œæˆ
            response_data = event.get("response", {})
            usage_data = response_data.get("usage", {})
            
            return {
                "id": f"chatcmpl-codex-{request_id}",
                "object": "chat.completion.chunk",
                "created": int(time.time()),
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {},
                    "finish_reason": "stop"
                }],
                "usage": {
                    "prompt_tokens": usage_data.get("input_tokens", 0),
                    "completion_tokens": usage_data.get("output_tokens", 0),
                    "total_tokens": usage_data.get("total_tokens", 0)
                }
            }
        
        return None


async def fetch_models_from_codex(access_token: str, account_id: str = "") -> List[Dict[str, str]]:
    """
    ä» Codex API åŠ¨æ€è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
    
    Args:
        access_token: OAuth access token
        account_id: ChatGPT account ID
    
    Returns:
        æ¨¡å‹åˆ—è¡¨ï¼Œå¤±è´¥åˆ™è¿”å›é™æ€åˆ—è¡¨
    """
    try:
        headers = get_codex_headers(access_token, account_id)
        # å°è¯•ä» Codex API è·å–æ¨¡å‹åˆ—è¡¨
        # éœ€è¦ client_version æŸ¥è¯¢å‚æ•°
        models_url = f"{CODEX_API_BASE}/models"
        params = {"client_version": "0.50.0"}
        
        print(f"[Codex Client] ğŸ” è¯·æ±‚æ¨¡å‹åˆ—è¡¨: {models_url}", flush=True)
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(models_url, headers=headers, params=params)
            
            if response.status_code == 200:
                data = response.json()
                models = []
                
                # è§£æå“åº”ï¼Œæå–æ¨¡å‹åˆ—è¡¨
                model_list = data.get("data", data.get("models", []))
                if isinstance(model_list, list):
                    for m in model_list:
                        model_id = m.get("id", "") if isinstance(m, dict) else str(m)
                        if model_id:
                            # åªä¿ç•™ gpt-5 ç³»åˆ—åŠæ›´é«˜ç‰ˆæœ¬
                            if any(prefix in model_id.lower() for prefix in ["gpt-5", "o3", "o4", "o5"]):
                                models.append({
                                    "id": model_id,
                                    "owned_by": m.get("owned_by", "openai") if isinstance(m, dict) else "openai"
                                })
                
                if models:
                    print(f"[Codex Client] âœ… åŠ¨æ€è·å–åˆ° {len(models)} ä¸ªæ¨¡å‹", flush=True)
                    return models
            else:
                response_text = response.text[:500] if response.text else "(empty)"
                print(f"[Codex Client] âš ï¸ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {response.status_code} - {response_text}", flush=True)
    
    except Exception as e:
        print(f"[Codex Client] âš ï¸ è·å–æ¨¡å‹åˆ—è¡¨å¼‚å¸¸: {e}", flush=True)
    
    # è¿”å›é™æ€æ¨¡å‹åˆ—è¡¨ä½œä¸ºåå¤‡
    return get_static_models()


def get_static_models() -> List[Dict[str, str]]:
    """
    è·å–é™æ€æ¨¡å‹åˆ—è¡¨ï¼ˆä½œä¸ºåå¤‡ï¼‰
    
    åŸºäº OpenAI Codex å®˜æ–¹æ–‡æ¡£çš„æ¨¡å‹åˆ—è¡¨
    ä½¿ç”¨ codex- å‰ç¼€æ–¹ä¾¿å®¢æˆ·ç«¯è¯†åˆ«
    
    æ”¯æŒæ€ç»´é“¾åç¼€:
    - -maxthinking: å¼ºæ¨ç†æ¨¡å¼ (reasoning.effort = "high")
    - -high: é«˜æ¨ç†æ¨¡å¼
    - -medium: ä¸­ç­‰æ¨ç†æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
    - -low: ä½æ¨ç†æ¨¡å¼
    - -nothinking: æ— æ¨ç†æ¨¡å¼
    """
    # åŸºç¡€æ¨¡å‹åˆ—è¡¨
    base_models = [
        # æ¨èæ¨¡å‹ (Recommended models)
        "gpt-5.2-codex",          # æœ€å…ˆè¿›çš„ä»£ç†ç¼–ç æ¨¡å‹
        "gpt-5.1-codex-mini",     # GPT-5.1-Codex çš„æ›´å°æ›´ç»æµç‰ˆæœ¬
        
        # æ›¿ä»£æ¨¡å‹ (Alternative models)
        "gpt-5.1-codex-max",      # é’ˆå¯¹é•¿æœŸä»£ç†ç¼–ç ä»»åŠ¡ä¼˜åŒ–
        "gpt-5.2",                # é€šç”¨ä»£ç†æ¨¡å‹
        "gpt-5.1",                # ç¼–ç å’Œä»£ç†ä»»åŠ¡
        "gpt-5.1-codex",          # é•¿æœŸä»£ç†ç¼–ç ä»»åŠ¡
        "gpt-5-codex",            # GPT-5 çš„é•¿æœŸä»£ç†ç¼–ç ç‰ˆæœ¬
        "gpt-5-codex-mini",       # GPT-5-Codex çš„æ›´å°æ›´ç»æµç‰ˆæœ¬
        "gpt-5",                  # ç¼–ç å’Œä»£ç†çš„æ¨ç†æ¨¡å‹
    ]
    
    # æ”¯æŒæ€ç»´é“¾åç¼€çš„æ¨¡å‹ï¼ˆåŒ…æ‹¬å¸¦ -codex åç¼€çš„ç‰ˆæœ¬ï¼‰
    models_with_thinking_suffixes = [
        "gpt-5", "gpt-5.1", "gpt-5.2",
        "gpt-5-codex", "gpt-5.1-codex", "gpt-5.2-codex",
        "gpt-5-codex-mini", "gpt-5.1-codex-mini",
        "gpt-5.1-codex-max",
    ]
    thinking_suffixes = ["-maxthinking", "-nothinking", "-low"]
    
    models = []
    
    for base in base_models:
        # åªæ·»åŠ å¸¦ codex- å‰ç¼€çš„ç‰ˆæœ¬
        models.append({"id": f"codex-{base}", "owned_by": "openai"})
        
        # ä¸ºæ”¯æŒçš„æ¨¡å‹æ·»åŠ æ€ç»´é“¾åç¼€å˜ä½“ï¼ˆåªä¿ç•™å¸¦ codex- å‰ç¼€çš„ï¼‰
        for supported in models_with_thinking_suffixes:
            if base == supported or base.startswith(supported):
                for suffix in thinking_suffixes:
                    models.append({"id": f"codex-{base}{suffix}", "owned_by": "openai"})
                break  # åªåŒ¹é…ä¸€æ¬¡
    
    return models


async def get_available_models(access_token: str = None, account_id: str = "") -> List[Dict[str, str]]:
    """
    è·å–å¯ç”¨çš„ Codex æ¨¡å‹åˆ—è¡¨
    
    ä¼˜å…ˆä» API åŠ¨æ€è·å–ï¼Œå¤±è´¥åˆ™ä½¿ç”¨é™æ€åˆ—è¡¨
    """
    if access_token:
        return await fetch_models_from_codex(access_token, account_id)
    return get_static_models()