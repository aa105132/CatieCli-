import httpx
import json
from typing import AsyncGenerator, Optional, Dict, Any
from app.config import settings


class GeminiClient:
    """Gemini API å®¢æˆ·ç«¯ - ä½¿ç”¨ Google å†…éƒ¨ API"""
    
    # å†…éƒ¨ API ç«¯ç‚¹
    INTERNAL_API_BASE = "https://cloudcode-pa.googleapis.com"
    
    def __init__(self, access_token: str, project_id: str = None):
        self.access_token = access_token
        self.project_id = project_id or ""
    
    async def generate_content(
        self,
        model: str,
        contents: list,
        generation_config: Optional[Dict] = None,
        system_instruction: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå†…å®¹ (éæµå¼) - ä½¿ç”¨å†…éƒ¨ API"""
        url = f"{self.INTERNAL_API_BASE}/v1internal:generateContent"
        
        headers = {
            "Authorization": f"Bearer {self.access_token}",
            "Content-Type": "application/json",
            "User-Agent": "catiecli/1.0",
        }
        
        # æ„å»ºå†…éƒ¨ API æ ¼å¼çš„ payload
        request_body = {"contents": contents}
        if generation_config:
            request_body["generationConfig"] = generation_config
        if system_instruction:
            request_body["systemInstruction"] = system_instruction
        
        # æ·»åŠ å®‰å…¨è®¾ç½®
        request_body["safetySettings"] = [
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "OFF"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "OFF"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "OFF"},
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "OFF"},
        ]
        
        # æ·»åŠ æœç´¢å·¥å…·é…ç½®ï¼ˆæ ¹æ®æ¨¡å‹åæ£€æµ‹ï¼‰
        use_search = "-search" in model
        if use_search:
            request_body["tools"] = [{"googleSearch": {}}]
            print(f"[GeminiClient] ğŸ” å·²å¯ç”¨æœç´¢åŠŸèƒ½ (googleSearch)", flush=True)
        
        # æ¸…ç†æ¨¡å‹åä¸­çš„åç¼€ï¼ˆAPI ä¸è¯†åˆ«è¿™äº›åç¼€ï¼‰
        api_model = model
        # å…ˆå¤„ç† -search åç¼€
        if use_search:
            api_model = api_model.replace("-maxthinking-search", "-maxthinking").replace("-nothinking-search", "-nothinking").replace("-search", "")
        # å†å¤„ç† -maxthinking/-nothinking åç¼€ï¼ˆè¿™äº›åç¼€ç”¨äº thinkingConfigï¼Œä½† API æ¨¡å‹åä¸èƒ½åŒ…å«ï¼‰
        api_model = api_model.replace("-maxthinking", "").replace("-nothinking", "")
        
        payload = {
            "model": api_model,
            "project": self.project_id,
            "request": request_body,
        }
        
        print(f"[GeminiClient] è¯·æ±‚: model={model}, project={self.project_id}", flush=True)
        print(f"[GeminiClient] generationConfig: {generation_config}", flush=True)
        
        # ä½¿ç”¨æ›´ç»†ç²’åº¦çš„è¶…æ—¶é…ç½®ï¼Œé¿å…é•¿æ—¶é—´ç”Ÿæˆæ—¶è¿æ¥ä¸­æ–­
        timeout = httpx.Timeout(
            connect=30.0,    # è¿æ¥è¶…æ—¶
            read=600.0,      # è¯»å–è¶…æ—¶ï¼ˆç­‰å¾…å“åº”ï¼‰
            write=30.0,      # å†™å…¥è¶…æ—¶
            pool=30.0        # è¿æ¥æ± è¶…æ—¶
        )
        async with httpx.AsyncClient(timeout=timeout) as client:
            response = await client.post(url, headers=headers, json=payload)
            
            # æ‰“å°æ‰€æœ‰å“åº”å¤´ï¼ˆè°ƒè¯•ç”¨ï¼‰
            print(f"[GeminiClient] å“åº”å¤´: {dict(response.headers)}", flush=True)
            
            if response.status_code != 200:
                error_text = response.text
                print(f"[GeminiClient] âŒ é”™è¯¯ {response.status_code}: {error_text[:500]}", flush=True)
                raise Exception(f"API Error {response.status_code}: {error_text}")
            result = response.json()
            # è°ƒè¯•ï¼šæ‰“å°åŸå§‹å“åº”
            print(f"[GeminiClient] âœ… åŸå§‹å“åº”: {json.dumps(result, ensure_ascii=False)[:1000]}", flush=True)
            return result
    
    async def generate_content_stream(
        self,
        model: str,
        contents: list,
        generation_config: Optional[Dict] = None,
        system_instruction: Optional[Dict] = None
    ) -> AsyncGenerator[str, None]:
        """ç”Ÿæˆå†…å®¹ (æµå¼) - ä½¿ç”¨å†…éƒ¨ API"""
        url = f"{self.INTERNAL_API_BASE}/v1internal:streamGenerateContent?alt=sse"
        
        headers = {
            "Authorization": f"Bearer {self.access_token}",
            "Content-Type": "application/json",
            "User-Agent": "catiecli/1.0",
        }
        
        # æ„å»ºå†…éƒ¨ API æ ¼å¼çš„ payload
        request_body = {"contents": contents}
        if generation_config:
            request_body["generationConfig"] = generation_config
        if system_instruction:
            request_body["systemInstruction"] = system_instruction
        
        # æ·»åŠ å®‰å…¨è®¾ç½®
        request_body["safetySettings"] = [
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "OFF"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "OFF"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "OFF"},
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "OFF"},
        ]
        
        # æ·»åŠ æœç´¢å·¥å…·é…ç½®ï¼ˆæ ¹æ®æ¨¡å‹åæ£€æµ‹ï¼‰
        use_search = "-search" in model
        if use_search:
            request_body["tools"] = [{"googleSearch": {}}]
            print(f"[GeminiClient] ğŸ” å·²å¯ç”¨æœç´¢åŠŸèƒ½ (googleSearch) - æµå¼", flush=True)
        
        # æ¸…ç†æ¨¡å‹åä¸­çš„åç¼€ï¼ˆAPI ä¸è¯†åˆ«è¿™äº›åç¼€ï¼‰
        api_model = model
        # å…ˆå¤„ç† -search åç¼€
        if use_search:
            api_model = api_model.replace("-maxthinking-search", "-maxthinking").replace("-nothinking-search", "-nothinking").replace("-search", "")
        # å†å¤„ç† -maxthinking/-nothinking åç¼€ï¼ˆè¿™äº›åç¼€ç”¨äº thinkingConfigï¼Œä½† API æ¨¡å‹åä¸èƒ½åŒ…å«ï¼‰
        api_model = api_model.replace("-maxthinking", "").replace("-nothinking", "")
        
        payload = {
            "model": api_model,
            "project": self.project_id,
            "request": request_body,
        }
        
        print(f"[GeminiClient] æµå¼è¯·æ±‚: model={model}, project={self.project_id}", flush=True)
        
        async with httpx.AsyncClient(timeout=120.0) as client:
            async with client.stream(
                "POST", url, headers=headers, json=payload
            ) as response:
                if response.status_code != 200:
                    error_text = await response.aread()
                    print(f"[GeminiClient] âŒ æµå¼é”™è¯¯ {response.status_code}: {error_text.decode()[:500]}", flush=True)
                    raise Exception(f"API Error {response.status_code}: {error_text.decode()}")
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        yield line[6:]
    
    async def fetch_quota_info(self) -> Dict[str, Any]:
        """è·å–é…é¢ä¿¡æ¯ - ä» Google API è·å–å®æ—¶é…é¢
        
        Returns:
            {
                "success": True/False,
                "models": {
                    "model_name": {
                        "remaining": 0.95,  # å‰©ä½™æ¯”ä¾‹ (0-1)
                        "resetTime": "2026-01-17T15:00:00Z"
                    }
                },
                "error": "é”™è¯¯ä¿¡æ¯" (ä»…åœ¨å¤±è´¥æ—¶)
            }
        """
        # æ³¨æ„ï¼šGeminiCLI å‡­è¯æ— æ³•æŸ¥è¯¢å®æ—¶é…é¢
        # - cloudcode-pa.googleapis.com è¿”å› 403 (æ— æƒé™)
        # - antigravity.googleapis.com è¿”å› 404 (ä¸æ¥å— GeminiCLI token)
        # æ­¤æ–¹æ³•ä»…ä½œä¸ºå°è¯•ï¼Œå¤±è´¥åä¼šé™çº§åˆ°æœ¬åœ°ç»Ÿè®¡
        url = f"{self.INTERNAL_API_BASE}/v1internal:fetchAvailableModels"
        
        headers = {
            "Authorization": f"Bearer {self.access_token}",
            "Content-Type": "application/json",
            "User-Agent": "antigravity/1.11.3 windows/amd64",  # å¿…é¡»ä½¿ç”¨ Antigravity User-Agent
            "Accept-Encoding": "gzip",
        }
        
        # æ„å»ºè¯·æ±‚ä½“ï¼ŒåŒ…å« project_id
        payload = {}
        if self.project_id:
            payload["project"] = self.project_id
        
        print(f"[GeminiClient] fetch_quota_info: project={self.project_id}", flush=True)
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(url, headers=headers, json=payload)
                
                print(f"[GeminiClient] fetch_quota_info å“åº”çŠ¶æ€: {response.status_code}", flush=True)
                
                if response.status_code == 200:
                    data = response.json()
                    print(f"[GeminiClient] fetch_quota_info å“åº”å†…å®¹: {json.dumps(data, ensure_ascii=False)[:800]}", flush=True)
                    quota_info = {}
                    
                    if 'models' in data and isinstance(data['models'], dict):
                        for model_id, model_data in data['models'].items():
                            if isinstance(model_data, dict) and 'quotaInfo' in model_data:
                                quota = model_data['quotaInfo']
                                remaining = quota.get('remainingFraction', 0)
                                reset_time = quota.get('resetTime', '')
                                
                                quota_info[model_id] = {
                                    "remaining": remaining,
                                    "resetTime": reset_time
                                }
                    
                    print(f"[GeminiClient] fetch_quota_info è§£æåˆ° {len(quota_info)} ä¸ªæ¨¡å‹é…é¢", flush=True)
                    return {"success": True, "models": quota_info}
                else:
                    error_text = response.text[:500]
                    print(f"[GeminiClient] fetch_quota_info å¤±è´¥: {response.status_code} - {error_text}", flush=True)
                    return {"success": False, "error": f"APIè¿”å›é”™è¯¯: {response.status_code} - {error_text}"}
        except Exception as e:
            print(f"[GeminiClient] fetch_quota_info å¼‚å¸¸: {e}", flush=True)
            return {"success": False, "error": str(e)}
    
    def is_fake_streaming(self, model: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å‡æµå¼ - gcli æœ‰å‡æµï¼Œæ²¡æœ‰å‡éæµ"""
        return model.startswith("å‡æµ/")
    
    async def chat_completions(
        self,
        model: str,
        messages: list,
        **kwargs
    ) -> Dict[str, Any]:
        """OpenAIå…¼å®¹çš„chat completions (éæµå¼)"""
        contents, system_instruction = self._convert_messages_to_contents(messages)
        generation_config = self._build_generation_config(model, kwargs)
        gemini_model = self._map_model_name(model)
        
        result = await self.generate_content(gemini_model, contents, generation_config, system_instruction)
        return self._convert_to_openai_response(result, model)
    
    async def chat_completions_stream(
        self,
        model: str,
        messages: list,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """OpenAIå…¼å®¹çš„chat completions (æµå¼)"""
        contents, system_instruction = self._convert_messages_to_contents(messages)
        generation_config = self._build_generation_config(model, kwargs)
        gemini_model = self._map_model_name(model)
        
        async for chunk in self.generate_content_stream(gemini_model, contents, generation_config, system_instruction):
            yield self._convert_to_openai_stream(chunk, model)
    
    async def chat_completions_fake_stream(
        self,
        model: str,
        messages: list,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """å‡æµå¼: å…ˆå‘å¿ƒè·³ï¼Œæ‹¿åˆ°å®Œæ•´å“åº”åä¸€æ¬¡æ€§è¾“å‡º"""
        import asyncio
        
        contents, system_instruction = self._convert_messages_to_contents(messages)
        generation_config = self._build_generation_config(model, kwargs)
        gemini_model = self._map_model_name(model)
        
        # å‘é€åˆå§‹ chunkï¼ˆç©ºå†…å®¹ï¼Œä¿æŒè¿æ¥ï¼‰
        initial_chunk = {
            "id": "chatcmpl-catiecli",
            "object": "chat.completion.chunk",
            "created": 0,
            "model": model,
            "choices": [{"index": 0, "delta": {"role": "assistant"}, "finish_reason": None}]
        }
        yield f"data: {json.dumps(initial_chunk)}\n\n"
        
        # åˆ›å»ºè¯·æ±‚ä»»åŠ¡
        request_task = asyncio.create_task(
            self.generate_content(gemini_model, contents, generation_config, system_instruction)
        )
        
        # æ¯2ç§’å‘é€å¿ƒè·³ï¼Œç›´åˆ°è¯·æ±‚å®Œæˆ
        heartbeat_chunk = {
            "id": "chatcmpl-catiecli",
            "object": "chat.completion.chunk",
            "created": 0,
            "model": model,
            "choices": [{"index": 0, "delta": {}, "finish_reason": None}]
        }
        
        while not request_task.done():
            await asyncio.sleep(2)
            if not request_task.done():
                yield f"data: {json.dumps(heartbeat_chunk)}\n\n"
        
        # è·å–å®Œæ•´å“åº”
        try:
            result = await request_task
            content = ""
            
            # å†…éƒ¨ API è¿”å›æ ¼å¼æ˜¯ {"response": {"candidates": ...}}
            response_data = result.get("response", result)
            
            if "candidates" in response_data and response_data["candidates"]:
                candidate = response_data["candidates"][0]
                if "content" in candidate and "parts" in candidate["content"]:
                    for part in candidate["content"]["parts"]:
                        if "text" in part and not part.get("thought", False):
                            content += part.get("text", "")
            
            # è¾“å‡ºå®Œæ•´å†…å®¹
            if content:
                content_chunk = {
                    "id": "chatcmpl-catiecli",
                    "object": "chat.completion.chunk",
                    "created": 0,
                    "model": model,
                    "choices": [{"index": 0, "delta": {"content": content}, "finish_reason": None}]
                }
                yield f"data: {json.dumps(content_chunk)}\n\n"
            
            # å‘é€ç»“æŸæ ‡è®°
            done_chunk = {
                "id": "chatcmpl-catiecli",
                "object": "chat.completion.chunk",
                "created": 0,
                "model": model,
                "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}]
            }
            yield f"data: {json.dumps(done_chunk)}\n\n"
            yield "data: [DONE]\n\n"
            
        except Exception as e:
            error_chunk = {
                "id": "chatcmpl-catiecli",
                "object": "chat.completion.chunk",
                "created": 0,
                "model": model,
                "choices": [{"index": 0, "delta": {"content": f"\n\n[Error: {str(e)}]"}, "finish_reason": "stop"}]
            }
            yield f"data: {json.dumps(error_chunk)}\n\n"
            yield "data: [DONE]\n\n"
    
    def _build_generation_config(self, model: str, kwargs: dict) -> dict:
        """æ„å»ºç”Ÿæˆé…ç½®ï¼ˆåŒ…å« thinking é…ç½®ï¼‰"""
        generation_config = {}
        
        # åŸºç¡€é…ç½®
        if "temperature" in kwargs:
            generation_config["temperature"] = kwargs["temperature"]
        if "max_tokens" in kwargs:
            generation_config["maxOutputTokens"] = kwargs["max_tokens"]
        if "top_p" in kwargs:
            generation_config["topP"] = kwargs["top_p"]
        if "top_k" in kwargs:
            top_k_value = kwargs["top_k"]
            # é˜²å‘†è®¾è®¡ï¼štopK æœ‰æ•ˆèŒƒå›´ä¸º 1-64ï¼ˆGemini CLI API æ”¯æŒèŒƒå›´ä¸º 1 inclusive åˆ° 65 exclusiveï¼‰
            # å½“ topK ä¸º 0 æˆ–æ— æ•ˆå€¼æ—¶ï¼Œä½¿ç”¨æœ€å¤§é»˜è®¤å€¼ 64ï¼›è¶…è¿‡ 64 æ—¶ä¹Ÿé”å®šä¸º 64
            if top_k_value is not None:
                if top_k_value < 1 or top_k_value > 64:
                    print(f"[GeminiClient] âš ï¸ topK={top_k_value} è¶…å‡ºæœ‰æ•ˆèŒƒå›´(1-64)ï¼Œå·²è‡ªåŠ¨è°ƒæ•´ä¸º 64", flush=True)
                    top_k_value = 64
            generation_config["topK"] = top_k_value
        
        # é»˜è®¤ topK (é¿å…æŸäº›æ¨¡å‹é—®é¢˜)
        if "topK" not in generation_config:
            generation_config["topK"] = 64
        
        # Thinking é…ç½®
        thinking_config = self._get_thinking_config(model)
        if thinking_config:
            generation_config.update(thinking_config)
        
        return generation_config
    
    def _convert_messages_to_contents(self, messages: list) -> tuple:
        """å°†OpenAIæ¶ˆæ¯æ ¼å¼è½¬æ¢ä¸ºGemini contentsæ ¼å¼
        
        Returns:
            (contents, system_instruction): contents åˆ—è¡¨å’Œç³»ç»ŸæŒ‡ä»¤å­—å…¸
        """
        contents = []
        system_instructions = []
        
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            if role == "system":
                # system å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨
                if isinstance(content, str):
                    system_instructions.append(content)
                elif isinstance(content, list):
                    for item in content:
                        if isinstance(item, dict) and item.get("type") == "text":
                            system_instructions.append(item.get("text", ""))
                        elif isinstance(item, str):
                            system_instructions.append(item)
                continue
            
            gemini_role = "user" if role == "user" else "model"
            
            # å¤„ç†å¤šæ¨¡æ€å†…å®¹ï¼ˆå›¾ç‰‡+æ–‡æœ¬ï¼‰
            parts = []
            if isinstance(content, str):
                # ç®€å•æ–‡æœ¬
                parts.append({"text": content})
            elif isinstance(content, list):
                # å¤šæ¨¡æ€å†…å®¹åˆ—è¡¨
                for item in content:
                    if isinstance(item, dict):
                        # OpenAI æ ¼å¼: {"type": "text", "text": "..."}
                        if item.get("type") == "text":
                            parts.append({"text": item.get("text", "")})
                        elif item.get("type") == "image_url":
                            # å¤„ç†å›¾ç‰‡
                            image_url = item.get("image_url", {})
                            url = image_url.get("url", "") if isinstance(image_url, dict) else image_url
                            if url.startswith("data:"):
                                # Base64 ç¼–ç çš„å›¾ç‰‡
                                # æ ¼å¼: data:image/jpeg;base64,/9j/4AAQ...
                                try:
                                    header, base64_data = url.split(",", 1)
                                    mime_type = header.split(":")[1].split(";")[0]
                                    parts.append({
                                        "inlineData": {
                                            "mimeType": mime_type,
                                            "data": base64_data
                                        }
                                    })
                                except Exception as e:
                                    print(f"[GeminiClient] âš ï¸ è§£æå›¾ç‰‡æ•°æ®å¤±è´¥: {e}", flush=True)
                            else:
                                # URL å›¾ç‰‡
                                parts.append({
                                    "fileData": {
                                        "mimeType": "image/jpeg",
                                        "fileUri": url
                                    }
                                })
                        # Gemini åŸç”Ÿæ ¼å¼: {"text": "..."} æˆ– {"inlineData": {...}} æˆ– {"fileData": {...}}
                        elif "text" in item and "type" not in item:
                            parts.append({"text": item["text"]})
                        elif "inlineData" in item:
                            parts.append({"inlineData": item["inlineData"]})
                        elif "fileData" in item:
                            parts.append({"fileData": item["fileData"]})
                        else:
                            # æœªçŸ¥æ ¼å¼ï¼Œå°è¯•ä½œä¸ºæ–‡æœ¬å¤„ç†
                            print(f"[GeminiClient] âš ï¸ æœªçŸ¥å†…å®¹æ ¼å¼: {list(item.keys())}", flush=True)
                    elif isinstance(item, str):
                        parts.append({"text": item})
            
            if not parts:
                parts.append({"text": ""})
            
            contents.append({
                "role": gemini_role,
                "parts": parts
            })
        
        # æ„å»º systemInstruction
        system_instruction = None
        if system_instructions:
            combined = "\n\n".join(system_instructions)
            system_instruction = {"parts": [{"text": combined}]}
        
        # å¦‚æœ contents ä¸ºç©ºï¼Œæ·»åŠ é»˜è®¤ç”¨æˆ·æ¶ˆæ¯
        if not contents:
            contents.append({"role": "user", "parts": [{"text": "è¯·æ ¹æ®ç³»ç»ŸæŒ‡ä»¤å›ç­”ã€‚"}]})
        
        return contents, system_instruction
    
    def _map_model_name(self, model: str) -> str:
        """æ˜ å°„æ¨¡å‹åç§° - åªæ¸…ç†å‰ç¼€ï¼Œä¿ç•™åç¼€ï¼ˆ-search, -maxthinking ç­‰ï¼‰ä¾› generate_content ä½¿ç”¨"""
        # ç§»é™¤å‰ç¼€ï¼ˆå‡æµ/æµå¼æŠ—æˆªæ–­ï¼‰- gcli æœ‰å‡æµï¼Œæ²¡æœ‰å‡éæµ
        stream_prefixes = ["å‡æµ/", "æµå¼æŠ—æˆªæ–­/"]
        for prefix in stream_prefixes:
            if model.startswith(prefix):
                model = model[len(prefix):]
                break
        
        # ç§»é™¤ API å‰ç¼€ï¼ˆgcli-/agy-ï¼‰
        api_prefixes = ["gcli-", "agy-"]
        for prefix in api_prefixes:
            if model.startswith(prefix):
                model = model[len(prefix):]
                break
        
        # OpenAI åˆ«åæ˜ å°„ï¼ˆåªå¯¹å®Œæ•´åŒ¹é…çš„åˆ«åç”Ÿæ•ˆï¼‰
        model_mapping = {
            "gpt-4": "gemini-2.5-pro",
            "gpt-4-turbo": "gemini-2.5-pro",
            "gpt-4o": "gemini-2.5-pro",
            "gpt-3.5-turbo": "gemini-2.5-flash",
            "claude-3-5-sonnet": "gemini-2.5-pro",
            "gemini-pro": "gemini-2.5-pro",
            "gemini-flash": "gemini-2.5-flash",
        }
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åˆ«åæ˜ å°„ï¼ˆå®Œæ•´åŒ¹é…ï¼‰
        if model in model_mapping:
            return model_mapping[model]
        
        # ä¿ç•™åç¼€ï¼ˆ-search, -maxthinking, -nothinkingï¼‰
        # è¿™äº›åç¼€ä¼šåœ¨ generate_content/generate_content_stream ä¸­å¤„ç†
        return model
    
    def _get_thinking_config(self, model: str) -> Optional[Dict]:
        """æ ¹æ®æ¨¡å‹åè·å– thinking é…ç½®"""
        # æ˜¾å¼æŒ‡å®š maxthinking
        if "-maxthinking" in model:
            if "flash" in model:
                return {"thinkingConfig": {"thinkingBudget": 24576, "includeThoughts": True}}
            return {"thinkingConfig": {"thinkingBudget": 32768, "includeThoughts": True}}
        # æ˜¾å¼æŒ‡å®š nothinking
        elif "-nothinking" in model:
            # flash æ¨¡å‹å¯ä»¥ç”¨ 0ï¼Œpro æ¨¡å‹æœ€ä½ 128
            if "flash" in model:
                return {"thinkingConfig": {"thinkingBudget": 0}}
            # pro/gemini-3 ç­‰é«˜çº§æ¨¡å‹æœ€ä½ 128
            return {"thinkingConfig": {"thinkingBudget": 128}}
        # gemini-3-pro-preview é»˜è®¤éœ€è¦ thinkingBudget
        elif "gemini-3" in model:
            return {"thinkingConfig": {"thinkingBudget": 8192, "includeThoughts": True}}
        # 2.5 pro ä¹Ÿå¯èƒ½éœ€è¦
        elif "2.5-pro" in model:
            return {"thinkingConfig": {"thinkingBudget": 1024, "includeThoughts": True}}
        return None
    
    # _get_search_config å·²åºŸå¼ƒï¼Œæœç´¢æ£€æµ‹ç›´æ¥åœ¨ generate_content ä¸­è¿›è¡Œ
    
    def _convert_to_openai_response(self, gemini_response: dict, model: str) -> dict:
        """å°†Geminiå“åº”è½¬æ¢ä¸ºOpenAIæ ¼å¼"""
        content = ""
        reasoning_content = ""
        
        # å†…éƒ¨ API è¿”å›æ ¼å¼æ˜¯ {"response": {"candidates": ...}}
        response_data = gemini_response.get("response", gemini_response)
        
        if "candidates" in response_data and response_data["candidates"]:
            candidate = response_data["candidates"][0]
            if "content" in candidate and "parts" in candidate["content"]:
                for part in candidate["content"]["parts"]:
                    text = part.get("text", "")
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ thinking å†…å®¹
                    if part.get("thought", False):
                        reasoning_content += text
                    else:
                        content += text
        
        message = {
            "role": "assistant",
            "content": content
        }
        # å¦‚æœæœ‰ reasoning å†…å®¹ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯ä¸­
        if reasoning_content:
            message["reasoning_content"] = reasoning_content
        
        return {
            "id": "chatcmpl-catiecli",
            "object": "chat.completion",
            "created": 0,
            "model": model,
            "choices": [{
                "index": 0,
                "message": message,
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0
            }
        }
    
    def _convert_to_openai_stream(self, chunk_data: str, model: str) -> str:
        """å°†Geminiæµå¼å“åº”è½¬æ¢ä¸ºOpenAI SSEæ ¼å¼"""
        try:
            data = json.loads(chunk_data)
            content = ""
            reasoning_content = ""
            
            # å†…éƒ¨ API è¿”å›æ ¼å¼æ˜¯ {"response": {"candidates": ...}}
            response_data = data.get("response", data)
            
            if "candidates" in response_data and response_data["candidates"]:
                candidate = response_data["candidates"][0]
                if "content" in candidate and "parts" in candidate["content"]:
                    for part in candidate["content"]["parts"]:
                        text = part.get("text", "")
                        if part.get("thought", False):
                            reasoning_content += text
                        else:
                            content += text
            
            # æ„å»º delta
            delta = {}
            if content:
                delta["content"] = content
            if reasoning_content:
                delta["reasoning_content"] = reasoning_content
            
            if not delta:
                return ""
            
            openai_chunk = {
                "id": "chatcmpl-catiecli",
                "object": "chat.completion.chunk",
                "created": 0,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": delta,
                    "finish_reason": None
                }]
            }
            return f"data: {json.dumps(openai_chunk)}\n\n"
        except:
            return ""
