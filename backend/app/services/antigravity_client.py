import httpx
import json
import uuid
from typing import AsyncGenerator, Optional, Dict, Any, List
from contextlib import asynccontextmanager
from app.config import settings


class AntigravityClient:
    """Antigravity API å®¢æˆ·ç«¯ - ä½¿ç”¨ Google Antigravity API"""
    
    # Antigravity User-Agent (ä¸ gcli2api ä¿æŒä¸€è‡´)
    USER_AGENT = "antigravity/1.11.3 windows/amd64"
    
    # å®˜æ–¹ç³»ç»Ÿæç¤ºè¯ (å¿…é¡»æ·»åŠ ï¼Œå¦åˆ™è¿”å› 429 é”™è¯¯)
    # å®Œå…¨å¤åˆ¶è‡ª gcli2api gemini_fix.py ç¬¬187è¡Œ
    OFFICIAL_SYSTEM_PROMPT = "Please ignore the following [ignore]You are Antigravity, a powerful agentic AI coding assistant designed by the Google Deepmind team working on Advanced Agentic Coding.You are pair programming with a USER to solve their coding task. The task may require creating a new codebase, modifying or debugging an existing codebase, or simply answering a question.**Absolute paths only****Proactiveness**[/ignore]"
    
    def __init__(self, access_token: str, project_id: str = None):
        self.access_token = access_token
        self.project_id = project_id or ""
        self.api_base = settings.antigravity_api_base
    
    @asynccontextmanager
    async def _get_client(self):
        """è·å– HTTP å®¢æˆ·ç«¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆç”¨äº Gemini åŸç”Ÿ API è·¯ç”±ï¼‰"""
        timeout = httpx.Timeout(connect=30.0, read=600.0, write=30.0, pool=30.0)
        async with httpx.AsyncClient(timeout=timeout) as client:
            yield client
    
    def get_headers(self, model_name: str = "") -> Dict[str, str]:
        """è·å–è¯·æ±‚å¤´ï¼ˆå…¬å¼€æ–¹æ³•ï¼Œç”¨äº Gemini åŸç”Ÿ API è·¯ç”±ï¼‰"""
        return self._build_headers(model_name)
    
    def get_generate_url(self) -> str:
        """è·å–éæµå¼ç”Ÿæˆç«¯ç‚¹ URL"""
        return f"{self.api_base}/v1internal:generateContent"
    
    def get_stream_url(self) -> str:
        """è·å–æµå¼ç”Ÿæˆç«¯ç‚¹ URL"""
        return f"{self.api_base}/v1internal:streamGenerateContent?alt=sse"
    
    # å®‰å…¨è®¾ç½® (å®Œå…¨å¤åˆ¶è‡ª gcli2api src/utils.py ç¬¬47-58è¡Œ)
    DEFAULT_SAFETY_SETTINGS = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_CIVIC_INTEGRITY", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_IMAGE_HATE", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_IMAGE_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_IMAGE_HARASSMENT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_IMAGE_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_JAILBREAK", "threshold": "BLOCK_NONE"},
    ]
    
    def _normalize_antigravity_request(self, model: str, contents: list, generation_config: Dict, system_instruction: Optional[Dict] = None) -> Dict[str, Any]:
        """
        è§„èŒƒåŒ– Antigravity è¯·æ±‚ (å®Œå…¨å¤åˆ¶ gcli2api gemini_fix.py normalize_gemini_request antigravity æ¨¡å¼)
        
        è¿”å›: {"model": str, "request": {...}}
        """
        result = {"contents": contents}
        
        # ========== 1. ç³»ç»Ÿæç¤ºè¯å¤„ç† (gemini_fix.py ç¬¬186-198è¡Œ) ==========
        existing_parts = []
        if system_instruction:
            if isinstance(system_instruction, dict):
                existing_parts = system_instruction.get("parts", [])
        
        # å¯é…ç½®çš„ç³»ç»Ÿæç¤ºè¯å‰ç¼€
        if settings.antigravity_system_prompt:
            existing_parts = [{"text": settings.antigravity_system_prompt}] + existing_parts
        
        # å®˜æ–¹æç¤ºè¯å§‹ç»ˆæ”¾åœ¨ç¬¬ä¸€ä½
        result["systemInstruction"] = {
            "parts": [{"text": self.OFFICIAL_SYSTEM_PROMPT}] + existing_parts
        }
        
        # ========== 1.5 å›¾ç‰‡æ¨¡å‹å¤„ç† (gemini_fix.py é€»è¾‘) ==========
        if "image" in model.lower():
            # å›¾ç‰‡ç”Ÿæˆæ¨¡å‹ç‰¹æ®Šå¤„ç†
            if "2k" in model.lower():
                final_model = "gemini-3-pro-image-2k"
                image_config = {"outputWidth": 2048, "outputHeight": 2048}
            elif "4k" in model.lower():
                final_model = "gemini-3-pro-image-4k"
                image_config = {"outputWidth": 4096, "outputHeight": 4096}
            else:
                final_model = "gemini-3-pro-image"
                image_config = {}  # é»˜è®¤åˆ†è¾¨ç‡
                
            generation_config = {
                "candidateCount": 1,
                "imageConfig": image_config
            }
            
            # æ¸…ç†ä¸å¿…è¦çš„å­—æ®µ
            result.pop("systemInstruction", None)
            
            return {
                "model": final_model,
                "request": {
                    "contents": contents,
                    "generationConfig": generation_config
                }
            }
        
        # ========== 2. æ€è€ƒæ¨¡å‹å¤„ç† (gemini_fix.py ç¬¬206-254è¡Œ) ==========
        is_thinking = "think" in model.lower() or "pro" in model.lower() or "claude" in model.lower()
        
        if is_thinking:
            if "thinkingConfig" not in generation_config:
                generation_config["thinkingConfig"] = {}
            
            thinking_config = generation_config["thinkingConfig"]
            if "thinkingBudget" not in thinking_config:
                thinking_config["thinkingBudget"] = 1024
            thinking_config["includeThoughts"] = True
            print(f"[AntigravityClient] å·²è®¾ç½® thinkingConfig: thinkingBudget={thinking_config['thinkingBudget']}", flush=True)
            
            # Claude æ¨¡å‹ç‰¹æ®Šå¤„ç†
            if "claude" in model.lower():
                # æ£€æµ‹æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ï¼ˆMCPåœºæ™¯ï¼‰
                has_tool_calls = any(
                    isinstance(content, dict) and 
                    any(
                        isinstance(part, dict) and ("functionCall" in part or "function_call" in part)
                        for part in content.get("parts", [])
                    )
                    for content in contents
                )
                
                if has_tool_calls:
                    print(f"[AntigravityClient] æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼ˆMCPåœºæ™¯ï¼‰ï¼Œç§»é™¤ thinkingConfig", flush=True)
                    generation_config.pop("thinkingConfig", None)
                else:
                    # é MCP åœºæ™¯ï¼šåœ¨æœ€åä¸€ä¸ª model æ¶ˆæ¯å¼€å¤´æ’å…¥æ€è€ƒå—
                    for i in range(len(contents) - 1, -1, -1):
                        content = contents[i]
                        if isinstance(content, dict) and content.get("role") == "model":
                            parts = content.get("parts", [])
                            thinking_part = {
                                "text": "...",
                                "thoughtSignature": "skip_thought_signature_validator"
                            }
                            if not parts or not (isinstance(parts[0], dict) and ("thought" in parts[0] or "thoughtSignature" in parts[0])):
                                content["parts"] = [thinking_part] + parts
                                print(f"[AntigravityClient] å·²æ’å…¥æ€è€ƒå— (thoughtSignature: skip_thought_signature_validator)", flush=True)
                            break
        
        # ========== 3. æ¨¡å‹åç§°æ˜ å°„ (gemini_fix.py ç¬¬256-274è¡Œ) ==========
        original_model = model
        model = model.replace("-thinking", "")
        
        model_lower = model.lower()
        if "opus" in model_lower:
            model = "claude-opus-4-5-thinking"
        elif "sonnet" in model_lower:
            model = "claude-sonnet-4-5-thinking"
        elif "haiku" in model_lower:
            model = "gemini-2.5-flash"
        elif "claude" in model_lower:
            model = "claude-sonnet-4-5-thinking"
        
        if original_model != model:
            print(f"[AntigravityClient] æ¨¡å‹æ˜ å°„: {original_model} -> {model}", flush=True)
        
        # ========== 4. ç§»é™¤ä¸æ”¯æŒçš„å­—æ®µ (gemini_fix.py ç¬¬276-278è¡Œ) ==========
        generation_config.pop("presencePenalty", None)
        generation_config.pop("frequencyPenalty", None)
        # Claude æ¨¡å‹å¯èƒ½ä¸æ”¯æŒ stopSequences
        if "claude" in model.lower():
            generation_config.pop("stopSequences", None)
            print(f"[AntigravityClient] Claude æ¨¡å‹å·²ç§»é™¤ stopSequences", flush=True)
        
        # ========== 5. å®‰å…¨è®¾ç½®å’Œå‚æ•°é™åˆ¶ (gemini_fix.py ç¬¬280-290è¡Œ) ==========
        result["safetySettings"] = self.DEFAULT_SAFETY_SETTINGS
        generation_config["maxOutputTokens"] = 64000
        generation_config["topK"] = 64
        
        # ========== 5.5 æœç´¢/è”ç½‘åŠŸèƒ½ ==========
        # å½“æ¨¡å‹ååŒ…å« -search æ—¶ï¼Œæ·»åŠ  googleSearch å·¥å…·
        if "-search" in original_model.lower():
            result["tools"] = [{"googleSearch": {}}]
            print(f"[AntigravityClient] å·²å¯ç”¨æœç´¢åŠŸèƒ½ (googleSearch)", flush=True)
            # ä»æ¨¡å‹åä¸­ç§»é™¤ -search åç¼€
            model = model.replace("-search", "")
        
        # ========== 6. Contents æ¸…ç† (gemini_fix.py ç¬¬292-342è¡Œ) ==========
        cleaned_contents = []
        for content in contents:
            if isinstance(content, dict) and "parts" in content:
                valid_parts = []
                for part in content["parts"]:
                    if not isinstance(part, dict):
                        continue
                    
                    has_valid_value = any(
                        value not in (None, "", {}, [])
                        for key, value in part.items()
                        if key != "thought"
                    )
                    
                    if has_valid_value:
                        part = part.copy()
                        if "text" in part:
                            text_value = part["text"]
                            if isinstance(text_value, list):
                                part["text"] = " ".join(str(t) for t in text_value if t)
                            elif isinstance(text_value, str):
                                part["text"] = text_value.rstrip()
                            else:
                                part["text"] = str(text_value)
                        valid_parts.append(part)
                
                if valid_parts:
                    cleaned_content = content.copy()
                    cleaned_content["parts"] = valid_parts
                    cleaned_contents.append(cleaned_content)
            else:
                cleaned_contents.append(content)
        
        result["contents"] = cleaned_contents
        result["generationConfig"] = generation_config
        
        return {"model": model, "request": result}
    
    def _build_headers(self, model_name: str = "") -> Dict[str, str]:
        """æ„å»º Antigravity API è¯·æ±‚å¤´"""
        headers = {
            "User-Agent": self.USER_AGENT,
            "Authorization": f"Bearer {self.access_token}",
            "Content-Type": "application/json",
            "Accept-Encoding": "gzip",
            "requestId": f"req-{uuid.uuid4()}",
        }
        
        # æ ¹æ®æ¨¡å‹åç§°åˆ¤æ–­ request_type
        if model_name:
            if "image" in model_name.lower():
                headers["requestType"] = "image_gen"
            else:
                headers["requestType"] = "agent"
        
        return headers
    
    async def generate_content(
        self,
        model: str,
        contents: list,
        generation_config: Optional[Dict] = None,
        system_instruction: Optional[Dict] = None,
        tools: Optional[List] = None,
        tool_config: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå†…å®¹ (éæµå¼) - ä½¿ç”¨ Antigravity APIï¼Œå®Œå…¨å¤åˆ¶ gcli2api é€»è¾‘"""
        url = f"{self.api_base}/v1internal:generateContent"
        
        # ä½¿ç”¨ gcli2api å®Œæ•´å¤åˆ¶çš„ normalize_gemini_request
        from app.services.gemini_fix import normalize_gemini_request
        
        if generation_config is None:
            generation_config = {}
        
        # æ„å»º Gemini è¯·æ±‚æ ¼å¼
        gemini_request = {
            "model": model,
            "contents": contents,
            "generationConfig": generation_config,
        }
        if system_instruction:
            gemini_request["systemInstruction"] = system_instruction
        if tools:
            gemini_request["tools"] = tools
        if tool_config:
            gemini_request["toolConfig"] = tool_config
        
        # è°ƒç”¨ gcli2api å®Œæ•´çš„è§„èŒƒåŒ–å‡½æ•°
        normalized = await normalize_gemini_request(gemini_request, mode="antigravity")
        final_model = normalized.pop("model")
        
        headers = self._build_headers(final_model)
        
        payload = {
            "model": final_model,
            "project": self.project_id,
            "request": normalized,
        }
        
        print(f"[AntigravityClient] â˜…â˜…â˜…â˜…â˜… å…³é”®ä¿¡æ¯ â˜…â˜…â˜…â˜…â˜…", flush=True)
        print(f"[AntigravityClient] â˜… MODEL: {final_model}", flush=True)
        print(f"[AntigravityClient] â˜… PROJECT: {self.project_id}", flush=True)
        print(f"[AntigravityClient] â˜… URL: {url}", flush=True)
        print(f"[AntigravityClient] â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…", flush=True)
        print(f"[AntigravityClient] generationConfig: {normalized.get('generationConfig')}", flush=True)
        print(f"[AntigravityClient] systemInstruction é¦–ä¸ª part å‰100å­—ç¬¦: {str(normalized.get('systemInstruction', {}).get('parts', [{}])[0])[:100]}", flush=True)
        print(f"[AntigravityClient] contents æ•°é‡: {len(normalized.get('contents', []))}", flush=True)
        # æ‰“å°å®Œæ•´ payload
        import json as json_module
        print(f"[AntigravityClient] ===== å®Œæ•´ PAYLOAD (å‰5000å­—ç¬¦) =====", flush=True)
        print(json_module.dumps(payload, ensure_ascii=False, indent=2)[:5000], flush=True)
        print(f"[AntigravityClient] ===== END PAYLOAD =====", flush=True)
        
        # ä½¿ç”¨æ›´ç»†ç²’åº¦çš„è¶…æ—¶é…ç½®
        timeout = httpx.Timeout(
            connect=30.0,
            read=600.0,
            write=30.0,
            pool=30.0
        )
        async with httpx.AsyncClient(timeout=timeout) as client:
            response = await client.post(url, headers=headers, json=payload)
            
            if response.status_code != 200:
                error_text = response.text
                print(f"[AntigravityClient] âŒ é”™è¯¯ {response.status_code}: {error_text[:500]}", flush=True)
                raise Exception(f"API Error {response.status_code}: {error_text}")
            result = response.json()
            print(f"[AntigravityClient] âœ… å“åº”: {json.dumps(result, ensure_ascii=False)[:500]}", flush=True)
            return result
    
    async def generate_content_stream(
        self,
        model: str,
        contents: list,
        generation_config: Optional[Dict] = None,
        system_instruction: Optional[Dict] = None,
        tools: Optional[List] = None,
        tool_config: Optional[Dict] = None
    ) -> AsyncGenerator[str, None]:
        """ç”Ÿæˆå†…å®¹ (æµå¼) - ä½¿ç”¨ Antigravity APIï¼Œå®Œå…¨å¤åˆ¶ gcli2api é€»è¾‘"""
        url = f"{self.api_base}/v1internal:streamGenerateContent?alt=sse"
        
        # ä½¿ç”¨ gcli2api å®Œæ•´å¤åˆ¶çš„ normalize_gemini_request
        from app.services.gemini_fix import normalize_gemini_request
        
        if generation_config is None:
            generation_config = {}
        
        # æ„å»º Gemini è¯·æ±‚æ ¼å¼
        gemini_request = {
            "model": model,
            "contents": contents,
            "generationConfig": generation_config,
        }
        if system_instruction:
            gemini_request["systemInstruction"] = system_instruction
        if tools:
            gemini_request["tools"] = tools
        if tool_config:
            gemini_request["toolConfig"] = tool_config
        
        # è°ƒç”¨ gcli2api å®Œæ•´çš„è§„èŒƒåŒ–å‡½æ•°
        normalized = await normalize_gemini_request(gemini_request, mode="antigravity")
        final_model = normalized.pop("model")
        
        headers = self._build_headers(final_model)
        
        payload = {
            "model": final_model,
            "project": self.project_id,
            "request": normalized,
        }
        
        print(f"[AntigravityClient] æµå¼è¯·æ±‚: model={final_model}, project={self.project_id}", flush=True)
        
        import asyncio
        
        timeout = httpx.Timeout(connect=30.0, read=600.0, write=30.0, pool=30.0)
        async with httpx.AsyncClient(timeout=timeout) as client:
            async with client.stream("POST", url, headers=headers, json=payload) as response:
                if response.status_code != 200:
                    error_text = await response.aread()
                    print(f"[AntigravityClient] âŒ æµå¼é”™è¯¯ {response.status_code}: {error_text.decode()[:500]}", flush=True)
                    raise Exception(f"API Error {response.status_code}: {error_text.decode()}")
                
                # ä½¿ç”¨å¿ƒè·³æœºåˆ¶ï¼šå¦‚æœè¶…è¿‡ 10 ç§’æ²¡æœ‰æ”¶åˆ°æ•°æ®ï¼Œå‘é€ç©ºå¿ƒè·³
                heartbeat_interval = 10  # ç§’
                heartbeat_count = 0
                
                async def line_iterator():
                    async for line in response.aiter_lines():
                        yield line
                
                line_iter = line_iterator()
                
                while True:
                    try:
                        # å°è¯•åœ¨è¶…æ—¶æ—¶é—´å†…è·å–ä¸‹ä¸€è¡Œ
                        line = await asyncio.wait_for(
                            line_iter.__anext__(),
                            timeout=heartbeat_interval
                        )
                        
                        if line.startswith("data: "):
                            yield line[6:]
                    
                    except asyncio.TimeoutError:
                        # è¶…æ—¶ï¼Œå‘é€ç©ºçš„å¿ƒè·³ JSONï¼ˆç©º candidatesï¼‰
                        heartbeat_count += 1
                        heartbeat_chunk = {"candidates": [{"content": {"parts": [{"text": ""}], "role": "model"}}]}
                        yield json.dumps(heartbeat_chunk)
                        print(f"[AntigravityClient] ğŸ’“ æµå¼å¿ƒè·³ #{heartbeat_count} (ç­‰å¾…æ€è€ƒä¸­...)", flush=True)
                    
                    except StopAsyncIteration:
                        # è¿­ä»£å™¨ç»“æŸ
                        break
    
    async def stream_generate_content(
        self,
        model: str,
        contents: list,
        generation_config: Optional[Dict] = None,
        system_instruction: Optional[Dict] = None,
        tools: Optional[List] = None,
        tool_config: Optional[Dict] = None
    ) -> AsyncGenerator[bytes, None]:
        """ç”Ÿæˆå†…å®¹ (æµå¼) - è¿”å› bytes (SSE æ ¼å¼)ï¼Œç”¨äº Anthropic ç­‰æ ¼å¼è½¬æ¢"""
        async for chunk in self.generate_content_stream(
            model=model,
            contents=contents,
            generation_config=generation_config,
            system_instruction=system_instruction,
            tools=tools,
            tool_config=tool_config
        ):
            yield f"data: {chunk}\n\n".encode('utf-8')
    
    async def fetch_available_models(self) -> List[Dict[str, Any]]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        url = f"{self.api_base}/v1internal:fetchAvailableModels"
        
        headers = self._build_headers()
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(url, headers=headers, json={})
                
                if response.status_code == 200:
                    data = response.json()
                    print(f"[AntigravityClient] æ¨¡å‹åˆ—è¡¨å“åº”: {json.dumps(data, ensure_ascii=False)[:500]}", flush=True)
                    
                    models = []
                    if 'models' in data and isinstance(data['models'], dict):
                        for model_id in data['models'].keys():
                            # è¿‡æ»¤æ‰å†…éƒ¨æµ‹è¯•æ¨¡å‹
                            model_lower = model_id.lower()
                            invalid_patterns = ["chat_", "rev", "tab_", "uic", "test", "exp", "lite_preview"]
                            if any(pattern in model_lower for pattern in invalid_patterns):
                                continue
                            models.append({
                                "id": model_id,
                                "object": "model",
                                "owned_by": "google"
                            })
                    print(f"[AntigravityClient] è·å–åˆ° {len(models)} ä¸ªæœ‰æ•ˆæ¨¡å‹", flush=True)
                    return models
                else:
                    print(f"[AntigravityClient] âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥ ({response.status_code}): {response.text[:500]}", flush=True)
                    return []
        except Exception as e:
            print(f"[AntigravityClient] âŒ è·å–æ¨¡å‹åˆ—è¡¨å¼‚å¸¸: {e}", flush=True)
            return []
    
    async def fetch_quota_info(self) -> Dict[str, Any]:
        """è·å–é…é¢ä¿¡æ¯"""
        from datetime import datetime, timezone
        
        url = f"{self.api_base}/v1internal:fetchAvailableModels"
        
        headers = self._build_headers()
        
        # æ„å»ºè¯·æ±‚ä½“ï¼ŒåŒ…å« project_idï¼ˆå¦‚æœæœ‰ï¼‰
        payload = {}
        if self.project_id:
            payload["project"] = self.project_id
        
        print(f"[AntigravityClient] fetch_quota_info: project={self.project_id}, url={url}", flush=True)
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(url, headers=headers, json=payload)
                
                print(f"[AntigravityClient] fetch_quota_info å“åº”çŠ¶æ€: {response.status_code}", flush=True)
                
                if response.status_code == 200:
                    data = response.json()
                    print(f"[AntigravityClient] fetch_quota_info å“åº”å†…å®¹: {json.dumps(data, ensure_ascii=False)[:800]}", flush=True)
                    quota_info = {}
                    min_reset_days = None  # ç”¨äºåˆ¤æ–­è´¦å·ç±»å‹
                    
                    if 'models' in data and isinstance(data['models'], dict):
                        for model_id, model_data in data['models'].items():
                            if isinstance(model_data, dict) and 'quotaInfo' in model_data:
                                quota = model_data['quotaInfo']
                                remaining = quota.get('remainingFraction', 0)
                                reset_time = quota.get('resetTime', '')
                                
                                # è®¡ç®—è·ç¦»é‡ç½®çš„å¤©æ•°
                                reset_days = None
                                if reset_time:
                                    try:
                                        # è§£æ ISO æ ¼å¼æ—¶é—´: 2025-01-25T00:00:00.000Z
                                        reset_dt = datetime.fromisoformat(reset_time.replace('Z', '+00:00'))
                                        now = datetime.now(timezone.utc)
                                        delta = reset_dt - now
                                        reset_days = max(0, delta.days)
                                        
                                        # è®°å½•æœ€å°é‡ç½®å¤©æ•°
                                        if min_reset_days is None or reset_days < min_reset_days:
                                            min_reset_days = reset_days
                                    except Exception as e:
                                        print(f"[AntigravityClient] è§£æ resetTime å¤±è´¥: {reset_time}, {e}", flush=True)
                                
                                quota_info[model_id] = {
                                    "remaining": remaining,
                                    "resetTime": reset_time,
                                    "resetDays": reset_days
                                }
                    
                    # åˆ¤æ–­è´¦å·ç±»å‹: PRO å·é‡ç½®å‘¨æœŸ <= 1 å¤©ï¼Œæ™®é€šå· 7 å¤©
                    is_pro = min_reset_days is not None and min_reset_days <= 1
                    account_tier = "pro" if is_pro else "normal"
                    
                    print(f"[AntigravityClient] fetch_quota_info è§£æåˆ° {len(quota_info)} ä¸ªæ¨¡å‹é…é¢, min_reset_days={min_reset_days}, tier={account_tier}", flush=True)
                    return {
                        "success": True, 
                        "models": quota_info,
                        "minResetDays": min_reset_days,
                        "accountTier": account_tier,
                        "isPro": is_pro
                    }
                else:
                    error_text = response.text[:500]
                    print(f"[AntigravityClient] fetch_quota_info å¤±è´¥: {response.status_code} - {error_text}", flush=True)
                    return {"success": False, "error": f"APIè¿”å›é”™è¯¯: {response.status_code} - {error_text}"}
        except Exception as e:
            print(f"[AntigravityClient] fetch_quota_info å¼‚å¸¸: {e}", flush=True)
            return {"success": False, "error": str(e)}
    
    def is_fake_streaming(self, model: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä½¿ç”¨å‡æµå¼æ¨¡å¼ï¼ˆæ¨¡å‹åä»¥ å‡éæµ/ å¼€å¤´ï¼‰"""
        return model.startswith("å‡éæµ/")
    
    async def chat_completions(
        self,
        model: str,
        messages: list,
        **kwargs
    ) -> Dict[str, Any]:
        """OpenAIå…¼å®¹çš„chat completions (éæµå¼) - ä½¿ç”¨ gcli2api é£æ ¼è½¬æ¢"""
        # 1. æ„å»ºå®Œæ•´çš„ OpenAI è¯·æ±‚å¯¹è±¡
        gemini_model = self._map_model_name(model)
        print(f"[AntigravityClient] æ¨¡å‹åæ˜ å°„: {model} -> {gemini_model}", flush=True)
        
        # æå– server_base_url
        server_base_url = kwargs.pop("server_base_url", None)

        openai_request = {
            "model": gemini_model,
            "messages": messages,
            **kwargs
        }
        
        # 2. ä½¿ç”¨ gcli2api å®Œæ•´ç‰ˆè½¬æ¢å™¨å°† OpenAI æ ¼å¼è½¬æ¢ä¸º Gemini æ ¼å¼
        from app.services.openai2gemini_full import convert_openai_to_gemini_request
        gemini_dict = await convert_openai_to_gemini_request(openai_request)
        
        print(f"[AntigravityClient] OpenAI->Gemini è½¬æ¢å®Œæˆ, contentsæ•°é‡: {len(gemini_dict.get('contents', []))}", flush=True)
        
        # 3. æå–è½¬æ¢åçš„å­—æ®µ
        contents = gemini_dict.get("contents", [])
        generation_config = gemini_dict.get("generationConfig", {})
        system_instruction = gemini_dict.get("systemInstruction")
        
        # 4. è°ƒç”¨ generate_content (ä¼šåœ¨å†…éƒ¨è°ƒç”¨ _normalize_antigravity_request)
        result = await self.generate_content(gemini_model, contents, generation_config, system_instruction)
        return self._convert_to_openai_response(result, model, server_base_url)
    
    async def chat_completions_stream(
        self,
        model: str,
        messages: list,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """OpenAIå…¼å®¹çš„chat completions (æµå¼) - ä½¿ç”¨ gcli2api é£æ ¼è½¬æ¢"""
        # 1. æ„å»ºå®Œæ•´çš„ OpenAI è¯·æ±‚å¯¹è±¡
        gemini_model = self._map_model_name(model)
        
        # æå– server_base_url
        server_base_url = kwargs.pop("server_base_url", None)

        openai_request = {
            "model": gemini_model,
            "messages": messages,
            **kwargs
        }
        
        # 2. ä½¿ç”¨å®Œæ•´ç‰ˆè½¬æ¢å™¨
        from app.services.openai2gemini_full import convert_openai_to_gemini_request
        gemini_dict = await convert_openai_to_gemini_request(openai_request)
        
        # 3. æå–å­—æ®µ
        contents = gemini_dict.get("contents", [])
        generation_config = gemini_dict.get("generationConfig", {})
        system_instruction = gemini_dict.get("systemInstruction")
        
        async for chunk in self.generate_content_stream(gemini_model, contents, generation_config, system_instruction):
            yield self._convert_to_openai_stream(chunk, model, server_base_url)
    
    async def chat_completions_fake_stream(
        self,
        model: str,
        messages: list,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """å‡æµå¼: å…ˆå‘å¿ƒè·³ï¼Œæ‹¿åˆ°å®Œæ•´å“åº”åä¸€æ¬¡æ€§è¾“å‡º - ä½¿ç”¨ gcli2api é£æ ¼è½¬æ¢"""
        import asyncio
        
        # 1. æ„å»ºå®Œæ•´çš„ OpenAI è¯·æ±‚å¯¹è±¡
        gemini_model = self._map_model_name(model)
        
        openai_request = {
            "model": gemini_model,
            "messages": messages,
            **kwargs
        }
        
        # 2. ä½¿ç”¨å®Œæ•´ç‰ˆè½¬æ¢å™¨
        from app.services.openai2gemini_full import convert_openai_to_gemini_request
        gemini_dict = await convert_openai_to_gemini_request(openai_request)
        
        # 3. æå–å­—æ®µ
        contents = gemini_dict.get("contents", [])
        generation_config = gemini_dict.get("generationConfig", {})
        system_instruction = gemini_dict.get("systemInstruction")
        
        # å‘é€åˆå§‹ chunkï¼ˆç©ºå†…å®¹ï¼Œä¿æŒè¿æ¥ï¼‰
        initial_chunk = {
            "id": "chatcmpl-antigravity",
            "object": "chat.completion.chunk",
            "created": 0,
            "model": model,
            "choices": [{"index": 0, "delta": {"role": "assistant"}, "finish_reason": None}]
        }
        yield f"data: {json.dumps(initial_chunk)}\n\n"
        
        # åˆ›å»ºè¯·æ±‚ä»»åŠ¡
        request_task = asyncio.create_task(
            self.generate_content(gemini_model, contents, generation_config, system_instruction)
        )
        
        # æ¯2ç§’å‘é€å¿ƒè·³ï¼Œç›´åˆ°è¯·æ±‚å®Œæˆ
        heartbeat_chunk = {
            "id": "chatcmpl-antigravity",
            "object": "chat.completion.chunk",
            "created": 0,
            "model": model,
            "choices": [{"index": 0, "delta": {}, "finish_reason": None}]
        }
        
        while not request_task.done():
            await asyncio.sleep(2)
            if not request_task.done():
                yield f"data: {json.dumps(heartbeat_chunk)}\n\n"
        
        # è·å–å®Œæ•´å“åº”
        try:
            result = await request_task
            content = ""
            
            # API è¿”å›æ ¼å¼æ˜¯ {"response": {"candidates": ...}}
            response_data = result.get("response", result)
            
            if "candidates" in response_data and response_data["candidates"]:
                candidate = response_data["candidates"][0]
                if "content" in candidate and "parts" in candidate["content"]:
                    parts = candidate["content"]["parts"]
                    text_parts = []
                    for part in parts:
                        if "text" in part and not part.get("thought", False):
                            text = part.get("text", "")
                            # è¿‡æ»¤æ‰ Gemini API çš„ç‰¹æ®Šæ ‡è®°ï¼ˆç²¾ç¡®åŒ¹é… <-XXX-> æ ¼å¼ï¼‰
                            import re
                            if text and not re.fullmatch(r'^<-[A-Z_]+->$', text.strip()):
                                text_parts.append(text)
                    # åˆå¹¶æ‰€æœ‰æ–‡æœ¬éƒ¨åˆ†ï¼Œå»é™¤æ¯ä¸ª part æœ«å°¾çš„å¤šä½™æ¢è¡Œç¬¦
                    # ä½†ä¿ç•™æ®µè½é—´çš„æ­£å¸¸æ¢è¡Œï¼ˆé€šè¿‡æ£€æµ‹æ˜¯å¦æ˜¯è‡ªç„¶å¥æœ«æ¢è¡Œï¼‰
                    if text_parts:
                        content = "".join(text_parts)
            
            # è¾“å‡ºå®Œæ•´å†…å®¹
            if content:
                content_chunk = {
                    "id": "chatcmpl-antigravity",
                    "object": "chat.completion.chunk",
                    "created": 0,
                    "model": model,
                    "choices": [{"index": 0, "delta": {"content": content}, "finish_reason": None}]
                }
                yield f"data: {json.dumps(content_chunk)}\n\n"
            
            # å‘é€ç»“æŸæ ‡è®°
            done_chunk = {
                "id": "chatcmpl-antigravity",
                "object": "chat.completion.chunk",
                "created": 0,
                "model": model,
                "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}]
            }
            yield f"data: {json.dumps(done_chunk)}\n\n"
            yield "data: [DONE]\n\n"
            
        except Exception as e:
            error_chunk = {
                "id": "chatcmpl-antigravity",
                "object": "chat.completion.chunk",
                "created": 0,
                "model": model,
                "choices": [{"index": 0, "delta": {"content": f"\n\n[Error: {str(e)}]"}, "finish_reason": "stop"}]
            }
            yield f"data: {json.dumps(error_chunk)}\n\n"
            yield "data: [DONE]\n\n"
    
    def _build_generation_config(self, model: str, kwargs: dict) -> dict:
        """æ„å»ºç”Ÿæˆé…ç½® (ä¸ gcli2api gemini_fix.py ä¿æŒä¸€è‡´)"""
        generation_config = {}
        
        # åŸºç¡€é…ç½® - åªä¿ç•™ç”¨æˆ·ä¼ å…¥çš„å‚æ•°ï¼Œå…¶ä»–å¼ºåˆ¶å‚æ•°åœ¨ _normalize_antigravity_request ä¸­å¤„ç†
        if "temperature" in kwargs:
            generation_config["temperature"] = kwargs["temperature"]
        if "top_p" in kwargs:
            generation_config["topP"] = kwargs["top_p"]
        
        return generation_config
    
    def _is_thinking_model(self, model: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæ€è€ƒæ¨¡å‹ (ä¸ gcli2api gemini_fix.py ç¬¬111-113è¡Œä¸€è‡´)"""
        return "think" in model.lower() or "pro" in model.lower() or "claude" in model.lower()
    
    def _apply_claude_thinking_fix(self, model: str, contents: list, generation_config: dict) -> None:
        """
        å¯¹ Claude æ¨¡å‹åº”ç”¨æ€è€ƒå—ä¿®å¤ (ä¸ gcli2api gemini_fix.py ç¬¬217-254è¡Œä¸€è‡´)
        
        å½“å­˜åœ¨å†å²å¯¹è¯æ—¶ï¼Œåœ¨æœ€åä¸€ä¸ª model æ¶ˆæ¯å¼€å¤´æ’å…¥å¸¦æœ‰
        thoughtSignature: skip_thought_signature_validator çš„æ€è€ƒå—
        """
        if "claude" not in model.lower():
            return
        
        # æ£€æµ‹æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ï¼ˆMCPåœºæ™¯ï¼‰
        has_tool_calls = any(
            isinstance(content, dict) and 
            any(
                isinstance(part, dict) and ("functionCall" in part or "function_call" in part)
                for part in content.get("parts", [])
            )
            for content in contents
        )
        
        if has_tool_calls:
            # MCP åœºæ™¯ï¼šæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œç§»é™¤ thinkingConfig
            print(f"[AntigravityClient] æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼ˆMCPåœºæ™¯ï¼‰ï¼Œç§»é™¤ thinkingConfig", flush=True)
            generation_config.pop("thinkingConfig", None)
        else:
            # é MCP åœºæ™¯ï¼šåœ¨æœ€åä¸€ä¸ª model æ¶ˆæ¯å¼€å¤´æ’å…¥æ€è€ƒå—
            for i in range(len(contents) - 1, -1, -1):
                content = contents[i]
                if isinstance(content, dict) and content.get("role") == "model":
                    parts = content.get("parts", [])
                    # ä½¿ç”¨å®˜æ–¹è·³è¿‡éªŒè¯çš„è™šæ‹Ÿç­¾å
                    thinking_part = {
                        "text": "...",
                        "thoughtSignature": "skip_thought_signature_validator"
                    }
                    # å¦‚æœç¬¬ä¸€ä¸ª part ä¸æ˜¯ thinkingï¼Œåˆ™æ’å…¥
                    if not parts or not (isinstance(parts[0], dict) and ("thought" in parts[0] or "thoughtSignature" in parts[0])):
                        content["parts"] = [thinking_part] + parts
                        print(f"[AntigravityClient] å·²åœ¨æœ€åä¸€ä¸ª assistant æ¶ˆæ¯å¼€å¤´æ’å…¥æ€è€ƒå—ï¼ˆå«è·³è¿‡éªŒè¯ç­¾åï¼‰", flush=True)
                    break
    
    def _convert_messages_to_contents(self, messages: list) -> tuple:
        """å°†OpenAIæ¶ˆæ¯æ ¼å¼è½¬æ¢ä¸ºGemini contentsæ ¼å¼"""
        contents = []
        system_instructions = []
        
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            if role == "system":
                if isinstance(content, str):
                    system_instructions.append(content)
                elif isinstance(content, list):
                    for item in content:
                        if isinstance(item, dict) and item.get("type") == "text":
                            system_instructions.append(item.get("text", ""))
                        elif isinstance(item, str):
                            system_instructions.append(item)
                continue
            
            gemini_role = "user" if role == "user" else "model"
            
            # å¤„ç†å¤šæ¨¡æ€å†…å®¹
            parts = []
            if isinstance(content, str):
                parts.append({"text": content})
            elif isinstance(content, list):
                for item in content:
                    if isinstance(item, dict):
                        if item.get("type") == "text":
                            parts.append({"text": item.get("text", "")})
                        elif item.get("type") == "image_url":
                            image_url = item.get("image_url", {})
                            url = image_url.get("url", "") if isinstance(image_url, dict) else image_url
                            if url.startswith("data:"):
                                try:
                                    header, base64_data = url.split(",", 1)
                                    mime_type = header.split(":")[1].split(";")[0]
                                    parts.append({
                                        "inlineData": {
                                            "mimeType": mime_type,
                                            "data": base64_data
                                        }
                                    })
                                except Exception as e:
                                    print(f"[AntigravityClient] âš ï¸ è§£æå›¾ç‰‡æ•°æ®å¤±è´¥: {e}", flush=True)
                            else:
                                parts.append({
                                    "fileData": {
                                        "mimeType": "image/jpeg",
                                        "fileUri": url
                                    }
                                })
                        elif "text" in item and "type" not in item:
                            parts.append({"text": item["text"]})
                        elif "inlineData" in item:
                            parts.append({"inlineData": item["inlineData"]})
                        elif "fileData" in item:
                            parts.append({"fileData": item["fileData"]})
                    elif isinstance(item, str):
                        parts.append({"text": item})
            
            if not parts:
                parts.append({"text": ""})
            
            contents.append({
                "role": gemini_role,
                "parts": parts
            })
        
        # æ„å»º systemInstruction
        system_instruction = None
        if system_instructions:
            combined = "\n\n".join(system_instructions)
            system_instruction = {"parts": [{"text": combined}]}
        
        if not contents:
            contents.append({"role": "user", "parts": [{"text": "è¯·æ ¹æ®ç³»ç»ŸæŒ‡ä»¤å›ç­”ã€‚"}]})
        
        return contents, system_instruction
    
    def _map_model_name(self, model: str) -> str:
        """æ˜ å°„æ¨¡å‹åç§° - åªåšå‰ç¼€å»é™¤ï¼ŒClaudeæ˜ å°„åœ¨ _normalize_antigravity_request ä¸­å®Œæˆ"""
        # ç§»é™¤ agy- å‰ç¼€ (CatieCli è‡ªå®šä¹‰)
        if model.startswith("agy-"):
            model = model[4:]
        # ç§»é™¤ gcli- å‰ç¼€ (å¦‚æœæœ‰)
        if model.startswith("gcli-"):
            model = model[5:]
        # ç§»é™¤ å‡éæµ/ å’Œ æµå¼æŠ—æˆªæ–­/ å‰ç¼€
        for prefix in ["å‡éæµ/", "æµå¼æŠ—æˆªæ–­/"]:
            if model.startswith(prefix):
                model = model[len(prefix):]
        
        return model
    
    def _convert_usage_metadata(self, usage_metadata: dict) -> dict:
        """
        å°†Geminiçš„usageMetadataè½¬æ¢ä¸ºOpenAIæ ¼å¼çš„usageå­—æ®µ
        
        Args:
            usage_metadata: Gemini APIçš„usageMetadataå­—æ®µ
        
        Returns:
            OpenAIæ ¼å¼çš„usageå­—å…¸
        """
        if not usage_metadata:
            return {
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0
            }
        
        return {
            "prompt_tokens": usage_metadata.get("promptTokenCount", 0),
            "completion_tokens": usage_metadata.get("candidatesTokenCount", 0),
            "total_tokens": usage_metadata.get("totalTokenCount", 0),
        }
    
    def _convert_to_openai_response(self, gemini_response: dict, model: str, server_base_url: str = None) -> dict:
        """å°†Geminiå“åº”è½¬æ¢ä¸ºOpenAIæ ¼å¼ - æ”¯æŒå·¥å…·è°ƒç”¨"""
        from app.services.openai2gemini_full import extract_tool_calls_from_parts
        
        content = ""
        reasoning_content = ""
        tool_calls = []
        finish_reason = "stop"
        
        response_data = gemini_response.get("response", gemini_response)
        
        # æå– usageMetadata
        usage_metadata = response_data.get("usageMetadata")
        
        if "candidates" in response_data and response_data["candidates"]:
            candidate = response_data["candidates"][0]
            gemini_finish_reason = candidate.get("finishReason", "STOP")
            
            if "content" in candidate and "parts" in candidate["content"]:
                parts = candidate["content"]["parts"]
                print(f"[AntigravityClient] å“åº” parts æ•°é‡: {len(parts)}, ç±»å‹: {[list(p.keys()) for p in parts]}", flush=True)
                
                # ä½¿ç”¨å®Œæ•´è½¬æ¢å™¨æå–å·¥å…·è°ƒç”¨
                tool_calls, text_content = extract_tool_calls_from_parts(parts, is_streaming=False)
                
                # å¤„ç†å…¶ä»–å†…å®¹
                for part in parts:
                    # å¤„ç†æ€è€ƒå†…å®¹
                    if "text" in part and part.get("thought", False):
                        reasoning_content += part.get("text", "")
                    # å¤„ç†æ™®é€šæ–‡æœ¬ (éæ€è€ƒ)
                    elif "text" in part and not part.get("thought", False):
                        text = part.get("text", "")
                        # è¿‡æ»¤æ‰ Gemini API çš„ç‰¹æ®Šæ ‡è®°ï¼ˆç²¾ç¡®åŒ¹é… <-XXX-> æ ¼å¼ï¼‰
                        import re
                        if text and not re.fullmatch(r'^<-[A-Z_]+->$', text.strip()):
                            content += text
                    # å¤„ç†å›¾ç‰‡ (inlineData)
                    elif "inlineData" in part:
                        inline_data = part["inlineData"]
                        mime_type = inline_data.get("mimeType", "image/png")
                        data = inline_data.get("data", "")
                        if data:
                            # ä¿å­˜å›¾ç‰‡åˆ°æœ¬åœ°å¹¶è·å– URL
                            from app.services.image_storage import ImageStorage
                            relative_url = ImageStorage.save_base64_image(data, mime_type)
                            
                            if relative_url:
                                if server_base_url:
                                    final_url = f"{server_base_url}{relative_url}"
                                else:
                                    final_url = relative_url
                                content += f"![Generated Image]({final_url})"
                            else:
                                data_url = f"data:{mime_type};base64,{data}"
                                content += f"![Generated Image]({data_url})"
                    # å¤„ç†ä»£ç æ‰§è¡Œ
                    elif "executableCode" in part:
                        exec_code = part["executableCode"]
                        lang = exec_code.get("language", "python").lower()
                        code = exec_code.get("code", "")
                        content += f"\n```{lang}\n{code}\n```\n"
                    # å¤„ç†ä»£ç æ‰§è¡Œç»“æœ
                    elif "codeExecutionResult" in part:
                        result = part["codeExecutionResult"]
                        outcome = result.get("outcome")
                        output = result.get("output", "")
                        if output:
                            label = "output" if outcome == "OUTCOME_OK" else "error"
                            content += f"\n```{label}\n{output}\n```\n"
            
            # ç¡®å®š finish_reason
            if tool_calls:
                if gemini_finish_reason == "STOP":
                    finish_reason = "tool_calls"
                elif gemini_finish_reason == "MAX_TOKENS":
                    finish_reason = "length"
                elif gemini_finish_reason in ["SAFETY", "RECITATION"]:
                    finish_reason = "content_filter"
            else:
                if gemini_finish_reason == "STOP":
                    finish_reason = "stop"
                elif gemini_finish_reason == "MAX_TOKENS":
                    finish_reason = "length"
                elif gemini_finish_reason in ["SAFETY", "RECITATION"]:
                    finish_reason = "content_filter"
        
        # æ„å»ºæ¶ˆæ¯
        message = {"role": "assistant"}
        
        if tool_calls:
            message["tool_calls"] = tool_calls
            message["content"] = content if content else None
            print(f"[AntigravityClient] âœ… æ£€æµ‹åˆ° {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨", flush=True)
        else:
            message["content"] = content
        
        if reasoning_content:
            message["reasoning_content"] = reasoning_content
        
        return {
            "id": "chatcmpl-antigravity",
            "object": "chat.completion",
            "created": 0,
            "model": model,
            "choices": [{
                "index": 0,
                "message": message,
                "finish_reason": finish_reason
            }],
            "usage": self._convert_usage_metadata(usage_metadata)
        }
    
    def _convert_to_openai_stream(self, chunk_data: str, model: str, server_base_url: str = None) -> str:
        """å°†Geminiæµå¼å“åº”è½¬æ¢ä¸ºOpenAI SSEæ ¼å¼ - æ”¯æŒå·¥å…·è°ƒç”¨å’Œusageç»Ÿè®¡"""
        try:
            from app.services.openai2gemini_full import extract_tool_calls_from_parts
            
            data = json.loads(chunk_data)
            content = ""
            reasoning_content = ""
            tool_calls = []
            finish_reason = None
            usage = None
            
            response_data = data.get("response", data)
            
            # æå– usageMetadataï¼ˆé€šå¸¸åœ¨æœ€åä¸€ä¸ª chunk ä¸­ï¼‰
            if "usageMetadata" in response_data:
                usage = self._convert_usage_metadata(response_data["usageMetadata"])
            
            if "candidates" in response_data and response_data["candidates"]:
                candidate = response_data["candidates"][0]
                gemini_finish_reason = candidate.get("finishReason")
                
                if "content" in candidate and "parts" in candidate["content"]:
                    parts = candidate["content"]["parts"]
                    
                    # ä½¿ç”¨å®Œæ•´è½¬æ¢å™¨æå–å·¥å…·è°ƒç”¨ (æµå¼éœ€è¦ index)
                    tool_calls, text_content = extract_tool_calls_from_parts(parts, is_streaming=True)
                    
                    for part in parts:
                        # å¤„ç†æ€è€ƒå†…å®¹
                        if "text" in part and part.get("thought", False):
                            reasoning_content += part.get("text", "")
                        # å¤„ç†æ™®é€šæ–‡æœ¬ (éæ€è€ƒï¼Œä¸”æœªè¢« extract_tool_calls_from_parts å¤„ç†)
                        elif "text" in part and not part.get("thought", False):
                            text = part.get("text", "")
                            # è¿‡æ»¤æ‰ Gemini API çš„ç‰¹æ®Šæ ‡è®°ï¼ˆç²¾ç¡®åŒ¹é… <-XXX-> æ ¼å¼ï¼‰
                            import re
                            if text and not re.fullmatch(r'^<-[A-Z_]+->$', text.strip()):
                                content += text
                        # å¤„ç†å›¾ç‰‡ (inlineData)
                        elif "inlineData" in part:
                            inline_data = part["inlineData"]
                            mime_type = inline_data.get("mimeType", "image/png")
                            img_data = inline_data.get("data", "")
                            if img_data:
                                from app.services.image_storage import ImageStorage
                                relative_url = ImageStorage.save_base64_image(img_data, mime_type)
                                
                                if relative_url:
                                    if server_base_url:
                                        final_url = f"{server_base_url}{relative_url}"
                                    else:
                                        final_url = relative_url
                                    content += f"![Generated Image]({final_url})"
                                else:
                                    data_url = f"data:{mime_type};base64,{img_data}"
                                    content += f"![Generated Image]({data_url})"
                        # å¤„ç†ä»£ç æ‰§è¡Œ
                        elif "executableCode" in part:
                            exec_code = part["executableCode"]
                            lang = exec_code.get("language", "python").lower()
                            code = exec_code.get("code", "")
                            content += f"\n```{lang}\n{code}\n```\n"
                        # å¤„ç†ä»£ç æ‰§è¡Œç»“æœ
                        elif "codeExecutionResult" in part:
                            result = part["codeExecutionResult"]
                            outcome = result.get("outcome")
                            output = result.get("output", "")
                            if output:
                                label = "output" if outcome == "OUTCOME_OK" else "error"
                                content += f"\n```{label}\n{output}\n```\n"
                
                # ç¡®å®š finish_reason
                if gemini_finish_reason:
                    if tool_calls and gemini_finish_reason == "STOP":
                        finish_reason = "tool_calls"
                    elif gemini_finish_reason == "STOP":
                        finish_reason = "stop"
                    elif gemini_finish_reason == "MAX_TOKENS":
                        finish_reason = "length"
                    elif gemini_finish_reason in ["SAFETY", "RECITATION"]:
                        finish_reason = "content_filter"
            
            # æ„å»º delta
            delta = {}
            if tool_calls:
                delta["tool_calls"] = tool_calls
            if content:
                delta["content"] = content
            if reasoning_content:
                delta["reasoning_content"] = reasoning_content
            
            if not delta and finish_reason is None:
                return ""
            
            openai_chunk = {
                "id": "chatcmpl-antigravity",
                "object": "chat.completion.chunk",
                "created": 0,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": delta,
                    "finish_reason": finish_reason
                }]
            }
            
            # åœ¨æœ€åä¸€ä¸ª chunk ä¸­æ·»åŠ  usage ä¿¡æ¯
            if usage:
                openai_chunk["usage"] = usage
            
            return f"data: {json.dumps(openai_chunk)}\n\n"
        except Exception as e:
            print(f"[AntigravityClient] âš ï¸ æµå¼è½¬æ¢å¼‚å¸¸: {e}", flush=True)
            return ""